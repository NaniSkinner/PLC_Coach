---
title: "Coaching Scenario: Achieving Consistent Scoring on Writing Assessments"
type: "coaching_scenario"
topics: ["scoring consistency", "rubrics", "writing assessment", "inter-rater reliability", "critical question 2"]
critical_question: [2]
grade_level: "elementary"
subject: "english_language_arts"
scenario_type: "Q2_scoring_consistency"
difficulty: "intermediate"
author: "AI PLC Coach"
publication_year: 2024
document_type: "coaching_scenario"
---

# Coaching Scenario: Achieving Consistent Scoring on Writing Assessments

## Context

Maria teaches 5th grade and her team has been using a common writing rubric for narrative essays. However, they've noticed significant scoring inconsistencies - the same student essay scored by different teachers gets scores ranging from 2 to 4 on the 4-point rubric. They need to calibrate their scoring to make their CFA data reliable.

**Team:** 5 fifth-grade teachers, good relationships, second year doing CFAs

**Problem:** 40% of papers scored differently when teachers trade papers, undermining data reliability

## The Coaching Conversation

### Turn 1: Teacher Opens

**Maria:** Our team has been using a common rubric for narrative writing all year, but we just discovered we're not scoring consistently. We had teachers trade papers and re-score 10 essays each, and 40% of them got different scores. One essay I scored as a 3, another teacher scored as a 2. How can we use data to drive decisions if we can't even score the same paper consistently?

**Coach Response:**

Maria, discovering scoring inconsistency is actually progress - many teams never check for this and make decisions based on unreliable data. Let me ask: When you and your colleague scored that essay differently, what made you say it was a 3 and what made her say it was a 2?

---

### Turn 2: Teacher Describes Disagreement

**Maria:** I said it was a 3 because the student had a clear beginning, middle, and end, good descriptive details, and mostly correct grammar. My colleague said it was a 2 because the rubric says level 3 requires "vivid, precise word choice" and this student used mostly simple, generic words like "nice" and "good."

When I looked at the rubric again, I realized we were focusing on different criteria. I was looking at organization and grammar. She was looking at word choice and voice.

**Coach Response:**

Perfect diagnosis! You've identified the problem: your rubric likely lists multiple criteria, and different teachers are weighting them differently. Teacher A sees strong organization and grammar and rounds up. Teacher B sees weak word choice and rounds down.

This is the most common reason for scoring inconsistency. The solution is to clarify what happens when a paper is strong in some criteria but weak in others. Let me show you how to build anchor papers that make these trade-offs explicit...

---

### Turns 3-8: [Conversation continues with coach helping Maria:]

- **Use anchor papers**: Select 6-8 student essays representing clear examples of each score level
- **Collaborative norming**: Team scores anchors together, discussing until consensus
- **Decision rules**: Explicit agreement on how to handle "split" papers (strong in some areas, weak in others)
- **Holistic vs. analytic rubrics**: When to use each type
- **Rubric revision**: Sometimes inconsistency reveals unclear rubric language
- **Regular recalibration**: Norm at least once per quarter to prevent scoring drift
- **Start of year anchors**: Use previous year's student work to calibrate before first CFA

**Outcome:** Maria's team commits to selecting anchor papers for levels 1-4, scoring them together, and documenting their reasoning. They realize their rubric needs revision - currently has 6 criteria making it hard to score holistically. They'll simplify to 3 key criteria aligned to their essential standard.

---

## Coaching Analysis

### Key Coaching Moves
- Diagnostic questioning to identify root cause (weighting criteria differently)
- Provided anchor paper protocol for building shared understanding
- Distinguished between rubric quality and scorer consistency issues
- Normalized scoring calibration as ongoing practice

### PLC Concepts
- Inter-rater reliability essential for trustworthy data
- Collaborative calibration builds shared understanding
- Rubrics need clear, observable criteria
- Anchor papers operationalize proficiency definitions

### Likely Outcomes
- Team will revise rubric for clarity
- Scoring consistency will improve significantly (from 60% to 85%+ agreement)
- Data-driven decisions will be based on reliable evidence
- Team will establish quarterly recalibration routine

## Reflection Questions
1. Why do teams often skip calibration work and how does that impact their PLC cycle?
2. How do anchor papers make abstract rubric language concrete?
3. What's the difference between rubric quality issues and scorer consistency issues?
