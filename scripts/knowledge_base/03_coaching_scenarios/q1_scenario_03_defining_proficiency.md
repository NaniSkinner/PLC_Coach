---
title: "Coaching Scenario: Defining Proficiency for Math Problem-Solving"
type: "coaching_scenario"
topics: ["proficiency criteria", "problem solving", "mathematics", "assessment", "critical question 1"]
critical_question: [1]
grade_level: "elementary"
subject: "mathematics"
scenario_type: "Q1_proficiency_definition"
difficulty: "intermediate"
author: "AI PLC Coach"
publication_year: 2024
document_type: "coaching_scenario"
---

# Coaching Scenario: Defining Proficiency for Math Problem-Solving

## Context

Jennifer is a 4th-grade teacher whose team has identified "Solve multi-step word problems using the four operations" as an essential standard. The team has created a common formative assessment with word problems, but after scoring their first CFA, they discovered their proficiency rates varied wildly - from 45% to 82% across four classrooms. When they compared student work, they realized they were scoring the same work differently.

**Team Composition:**
- 4 fourth-grade teachers
- School in third year of PLC implementation
- Strong consensus on essential standards
- Struggling with consistent assessment

**Current Situation:**
- Just administered first CFA on multi-step word problems
- Discovered 30+ percentage point gap in proficiency rates
- Team frustrated by inconsistent data
- Need to resolve scoring differences before next CFA
- Emergency team meeting called to address the problem

## The Coaching Conversation

### Turn 1: Teacher Opens

**Jennifer:** We have a serious problem. We gave our first common math assessment on word problems last week, and the results don't make sense. My proficiency rate was 68%, but another teacher had 82% and another had only 45%. At first we thought it was just differences in our students, but then we looked at the actual work.

We found student papers that look almost identical, but one teacher scored it proficient and another scored it below proficient. We're not on the same page at all about what proficiency means, and now we don't trust any of our data. How do we fix this?

**Coach's Purpose:** Validate frustration, normalize the problem, pivot to solution-focused approach.

**Coach Response:**

Jennifer, I appreciate your honesty about this, and here's the good news: you discovered this problem after ONE assessment instead of after a whole semester. Many teams don't realize they're scoring inconsistently until much later. The fact that you compared work across classrooms shows you're doing exactly what PLCs should do.

Inconsistent scoring almost always comes down to one thing: unclear or implicit criteria for proficiency. Let me ask you directly - when your team created this assessment, did you explicitly agree on what would count as proficient before you scored?

For example, on a multi-step word problem where students need to add and multiply:
- Do they have to get the final answer correct to be proficient?
- If they use the right operations but make a calculation error, is that proficient?
- Do they have to show their work?
- If they show work but it's disorganized, does that affect proficiency?

My guess is your team didn't explicitly answer these questions, so each person made their own judgment calls. Am I right?

---

### Turn 2: Teacher Confirms Problem and Shows Examples

**Jennifer:** Yes, exactly! We talked about the problems we wanted to include, but we never talked about how we'd score them. Everyone just scored based on what seemed right to them.

Let me show you a specific example. Here's a problem from our assessment:

*"Sarah has 4 bags of apples. Each bag has 6 apples. She gives 8 apples to her friend. How many apples does Sarah have left?"*

One teacher said proficient students had to get the right answer (16 apples). But I said if a student showed "4 × 6 = 24" and "24 - 8 = 16" with the correct operations, that's proficient even if they made a subtraction error and wrote 15 instead of 16. Another teacher said students had to also write a sentence explaining their thinking.

Who's right? How do we decide?

**Coach's Purpose:** Use this specific example to introduce systematic criteria development; separate content proficiency from other factors.

**Coach Response:**

This is a perfect example to work with. Let's unpack what your standard actually requires and what proficiency on THIS standard means. Your standard is "Solve multi-step word problems using the four operations." Let's identify what cognitive work the standard demands:

**Core Skills Required:**
1. Understand what the problem is asking
2. Identify which operations to use and in what sequence
3. Perform the calculations accurately
4. Interpret the result in context

Now here's the critical question: What is the ESSENTIAL skill your standard is targeting? Is it:
- **A) Mathematical reasoning** (knowing which operations and when)
- **B) Computational accuracy** (performing calculations correctly)
- **C) Communication** (explaining thinking)

Or some combination? Because your team is actually assessing different things:
- Teacher #1 is prioritizing computational accuracy (right answer only)
- You're prioritizing mathematical reasoning (right operations, calculation errors OK)
- Teacher #3 is including communication (written explanation required)

What does your standard emphasize? The phrase "solve multi-step word problems using the four operations" focuses on WHICH operations and WHEN to use them - that's reasoning, not just computation. So here's my suggestion:

**Proficiency should focus on:** Selecting correct operations and applying them in the right sequence
**Partial credit for:** Minor computational errors if operations are correct
**Not proficiency:** Wrong operations, even if computation is perfect

Does that distinction help clarify who's right?

---

### Turn 3: Teacher Sees Distinction but Has Follow-Up Questions

**Jennifer:** That really helps! So we're assessing problem-solving and operational thinking, not just whether kids can add and subtract accurately. If a student writes "4 × 6 = 24" and "24 - 8 = 15" (wrong answer but right approach), that's closer to proficient than a student who writes "4 + 6 + 8 = 18" (wrong operations entirely)?

But what about showing work? Some students in my class can solve these problems mentally and just write the answer. They clearly understand the operations, but they didn't show their thinking. Is that proficient?

**Coach's Purpose:** Affirm her growing understanding, address the "show your work" question, introduce explicit requirements vs. evidence of thinking.

**Coach Response:**

Yes! You've got the distinction now. The student who used correct operations but made a calculation error demonstrates more understanding of the standard than the student who used incorrect operations, even if that student somehow got lucky with the math.

Now for your "show your work" question - this is where teams get tricky. Let me separate two issues:

**Issue 1: Can students demonstrate proficiency without showing work?**
Sometimes, yes. If the answer is correct and the problem truly requires multi-step thinking, the correct answer is evidence they did the mental work. For example, in your apple problem, if a student writes "16 apples" with no work shown, they either:
- Got lucky (unlikely for multi-step)
- Did the operations mentally (demonstrates understanding)

You can't know for certain, but it's reasonable to accept correct answers as proficiency evidence.

**Issue 2: How do you know what a student understands when the answer is wrong?**
This is where showing work matters. If a student gets the wrong answer and shows no work, you have NO evidence of their thinking. You can't tell if they have operational understanding or not. That's not proficiency - it's insufficient evidence.

So here's a defensible policy:
- **Correct answer with or without work = Proficient** (answer is evidence of understanding)
- **Correct operations with minor errors = Proficient** (work shows operational understanding)
- **Wrong operations or no work = Below Proficient** (insufficient evidence of understanding)

Would that policy work for your team?

---

### Turn 4: Teacher Applies Framework to Other Items

**Jennifer:** Yes, that makes sense. So showing work isn't required for proficiency, but it's how students show evidence when their answer isn't perfect.

Let me test this with another problem from our assessment:

*"A bakery makes 48 cookies. They pack them into boxes of 6. They sell 5 boxes. How many cookies do they have left in boxes?"*

This is tricky because students have to:
1. Divide: 48 ÷ 6 = 8 boxes
2. Multiply: 5 boxes sold means 5 × 6 = 30 cookies sold
3. Subtract: 48 - 30 = 18 cookies left

OR they could:
1. Divide: 48 ÷ 6 = 8 boxes
2. Subtract: 8 - 5 = 3 boxes left
3. Multiply: 3 × 6 = 18 cookies left

Both approaches are valid, right? If a student uses either path and shows their work, that's proficient? Even if their path is different from mine?

**Coach's Purpose:** Affirm her analytical thinking, reinforce that multiple solution paths demonstrate deeper understanding, introduce how to score varied approaches.

**Coach Response:**

Jennifer, this is excellent! You've identified something really important: true mathematical proficiency often includes recognizing that multiple solution paths exist. Both approaches you described demonstrate understanding of the problem and correct use of operations.

In fact, students who recognize alternative solution paths often have STRONGER understanding than students who can only solve problems one way. So yes - either path, correctly executed, is proficient.

Here's what this means for scoring:

**Proficient includes:**
- Path 1: 48 ÷ 6 = 8, then 5 × 6 = 30, then 48 - 30 = 18 ✓
- Path 2: 48 ÷ 6 = 8, then 8 - 5 = 3, then 3 × 6 = 18 ✓
- Any other logical path with correct operations ✓

**Below Proficient:**
- Wrong operations (e.g., 48 + 6 + 5)
- Incomplete solution (stops after one step)
- Right operations in wrong sequence (e.g., subtracts before dividing)

Now here's the question: How will your team know if you're all scoring these varied approaches consistently? You need concrete examples. Have you done any calibration work with anchor papers?

---

### Turn 5: Teacher Asks About Calibration Process

**Jennifer:** No, we haven't. I've heard that term but I don't really know what it means. Is that like practice scoring?

And I'm also wondering - should we re-score all the assessments we already graded now that we're clearer on proficiency? Or do we just move forward with better criteria for the next assessment?

**Coach's Purpose:** Explain calibration process clearly, address whether to re-score, connect calibration to building shared understanding.

**Coach Response:**

Calibration is exactly like practice scoring, but more structured. Here's the protocol:

**Calibration Protocol:**

**Step 1: Select Sample Papers (10 minutes before meeting)**
Choose 8-10 student papers showing range of quality:
- Clear proficiency
- Clear below proficiency
- Borderline cases (these are most important!)

**Step 2: Individual Scoring (10 minutes)**
Each teacher independently scores all samples using your agreed criteria:
- Proficient or Below Proficient for each problem
- Brief notes about WHY

**Step 3: Compare Scores (20 minutes)**
Create chart showing how each teacher scored each paper
Notice where you agree (most papers, hopefully!)
Focus on disagreements

**Step 4: Discuss Borderline Cases (30 minutes)**
For each paper where people disagreed:
- Show the student work to everyone
- Each person explains their scoring decision
- Discuss until you reach consensus
- Document the decision (this becomes an anchor example)

**Step 5: Finalize Scoring Guide (10 minutes)**
Based on discussion, refine your proficiency criteria
Add specific examples from anchor papers

These anchors become your reference point for future scoring. When uncertain, ask: "Is this more like Anchor Paper A (proficient) or Anchor Paper B (below proficient)?"

**As for re-scoring:** I'd suggest yes, re-score. Here's why - right now you have data showing 45% to 82% proficiency, but you don't trust it. You need accurate baseline data. Have each teacher re-score their own papers using your new criteria. Then compare again. I bet the range narrows significantly.

Does that feel doable?

---

### Turn 6: Teacher Sees Value but Worries About Time

**Jennifer:** That protocol makes sense, and I agree we need to re-score to get accurate data. But I'm worried about time. Our team meetings are only 60 minutes, and we have so many other things to discuss - planning upcoming units, identifying intervention students, looking at data. How do we fit in calibration?

And honestly, this feels like something we should have done BEFORE we gave the assessment, not after. What's the right sequence for all of this?

**Coach's Purpose:** Validate time concern, provide realistic timeline, show how calibration actually saves time, clarify ideal sequence.

**Coach Response:**

You're absolutely right on both counts. Let me address each concern:

**Time Investment:** Yes, calibration takes time - about 60-75 minutes for thorough work. But consider what happens WITHOUT calibration:
- Unreliable data that leads to poor decisions
- Wasted time discussing data you don't trust
- Intervention plans based on inconsistent scoring
- Need to re-do work later (like you're doing now)

Calibration is an investment that pays off by making all your other data work trustworthy. I'd suggest dedicating ONE FULL meeting to calibration for each new assessment type. Once you're calibrated, scoring future assessments is faster and more consistent.

**Ideal Sequence:** You're right that calibration works best BEFORE giving the assessment. Here's the ideal sequence:

1. **Create assessment together** (focus on essential standard)
2. **Develop scoring criteria** (define proficiency explicitly)
3. **Calibrate with sample work** (could be from previous year)
4. **Administer assessment** (all teachers, same week)
5. **Score individually** (using calibrated criteria)
6. **Analyze data together** (confident in consistency)

You did steps 1 and 4, but skipped 2 and 3. That's normal for teams new to this. Now you're doing 2, 3, and 5 after the fact. Next assessment, you'll do it in order and save time.

Would your team be willing to invest one meeting in calibration for this assessment, knowing it will make future assessments smoother?

---

### Turn 7: Teacher Commits but Asks How to Present to Team

**Jennifer:** Yes, I think they would, especially when I explain why our data was all over the place. But I'm nervous about presenting this to my team. I don't want them to feel like I'm criticizing their scoring or saying they did it wrong. We're supposed to be collaborative, and this feels like I'm pointing out problems.

How do I bring this up in a way that doesn't make people defensive?

**Coach's Purpose:** Address interpersonal dynamics, provide language for framing the conversation collaboratively, emphasize shared problem.

**Coach Response:**

This is such an important question, Jennifer. The way you frame this conversation will determine whether people engage productively or get defensive. Here's language you might use:

**Opening Frame:**
"I've been thinking about our assessment results, and I noticed something interesting. Our proficiency rates ranged from 45% to 82%. At first I thought it was just our students, but when I looked at some papers, I found work that looked really similar scored differently. I think we never explicitly agreed on what proficiency meant for these word problems, so we each made our own decisions. I did some research on how teams solve this, and I learned about something called calibration. Would you be willing to try it with me?"

**Key Principles:**
1. **Use "we" not "you"** ("We didn't define criteria" not "You scored wrong")
2. **Share your own uncertainty** ("I wasn't sure how to score papers with right operations but wrong answers")
3. **Frame as learning opportunity** ("This will help us get more accurate data")
4. **Be curious, not critical** ("I'm wondering how we each thought about proficiency")

**If Someone Gets Defensive:**
"I'm not saying anyone scored wrong - I'm saying we never agreed on what right meant. That's a system problem, not a people problem. Let's fix the system together."

Does that framing feel comfortable to you?

---

### Turn 8: Teacher Feels Ready and Plans Next Steps

**Jennifer:** Yes, that helps a lot. You're right that framing it as "we" instead of pointing fingers will make a huge difference. I think my team will be relieved, actually, because I know the teacher with 45% proficiency has been feeling bad about her results. When she realizes it was partly scoring differences, not just her students struggling, that might actually help.

OK, here's my plan:
1. Start tomorrow's team meeting by sharing the problem: our proficiency rates don't make sense
2. Show a few examples of similar work scored differently (without saying whose scoring)
3. Propose that we calibrate using that protocol you gave me
4. Re-score our current assessments using consistent criteria
5. Set a norm that for future assessments, we'll define proficiency criteria BEFORE we give the test

Does that cover everything? And one more question - how often do teams need to calibrate? Is this a one-time thing or ongoing?

**Coach's Purpose:** Affirm plan, answer ongoing calibration question, connect to broader PLC cycle, offer support.

**Coach Response:**

Jennifer, your plan is excellent. The only addition I'd suggest: In step 3, when you propose calibration, emphasize that this is a normal PLC practice. You might say, "This is what high-functioning PLC teams do - they calibrate to make sure their data is reliable." That normalizes it and shows this isn't remedial work.

**How often to calibrate:**

**Initially:** Every new assessment type requires calibration
- First time assessing word problems: calibrate
- First time assessing fractions: calibrate
- First time assessing geometry: calibrate

**Periodically:** Once you've calibrated for a specific skill, you don't need full calibration every time, but you should:
- Do "spot checks" quarterly (everyone scores 3 same papers, compare)
- Recalibrate if you notice scoring drift
- Calibrate whenever you revise assessment significantly

**For new team members:** Always calibrate when someone joins the team - they need to understand your shared standards for proficiency.

Think of calibration like tuning instruments in an orchestra. You do it thoroughly at the start, then quick checks to stay in tune. After a while, you develop a shared ear for what proficiency sounds like.

I'm confident your meeting will go well. Would you like to debrief after the meeting to talk about how the calibration went?

---

## Coaching Analysis

### Effective Coaching Moves Used

**1. Normalized the Problem**
Coach immediately validated that inconsistent scoring is common and catching it early is actually positive.

**2. Diagnostic Questions**
Asked about the team's process to identify root cause (no pre-defined proficiency criteria).

**3. Concrete Examples**
Used Jennifer's specific assessment problems to illustrate concepts, making abstract ideas tangible.

**4. Separating Components**
Helped Jennifer distinguish between mathematical reasoning, computational accuracy, and communication - often conflated in math scoring.

**5. Clear Frameworks**
Provided specific protocols (calibration process, ideal sequence) that Jennifer could immediately implement.

**6. Interpersonal Coaching**
Addressed the softer skill of how to present difficult findings to team without creating defensiveness.

**7. Long-term Perspective**
Explained how calibration fits into ongoing PLC work, not just one-time fix.

### Key PLC Concepts Demonstrated

- **Common Formative Assessment:** Requires common understanding of proficiency, not just common problems
- **Calibration:** Structured process for developing shared scoring standards
- **Anchor Papers:** Concrete examples that define proficiency levels
- **Backward Design:** Should define proficiency criteria before assessment, not after
- **Collaborative Data Analysis:** Only meaningful with consistent, reliable data
- **Essential Standards Focus:** Clarity on what standard emphasizes (reasoning vs. computation)

### Likely Outcomes

If Jennifer implements this coaching advice:
- **Immediate:** Team will recalibrate and re-score current assessment with consistent criteria
- **Short-term:** Proficiency rate range will narrow significantly (likely 55-65% instead of 45-82%)
- **Medium-term:** Team will develop routine calibration practice for new assessments
- **Long-term:** Data-driven decisions will be more reliable; intervention will be more appropriately targeted

### Potential Challenges Ahead

1. **Time Pressure:** Team may resist dedicating full meeting to calibration
2. **Defensive Reactions:** Some teachers may feel criticized despite careful framing
3. **Computational Accuracy Debate:** Team may struggle with "right operations, wrong answer" scoring
4. **Consistency Drift:** Without periodic recalibration, scoring may become inconsistent again

The coach has positioned Jennifer well by providing both technical protocols and interpersonal strategies.

## Reflection Questions for Coaches

1. How did the coach help Jennifer distinguish between different dimensions of mathematical proficiency?

2. What evidence suggests Jennifer developed leadership capacity to facilitate her team's calibration process?

3. How did the coach balance acknowledging the problem with maintaining a solutions-focused approach?

4. What follow-up coaching might Jennifer need after her team's calibration meeting?

5. How might this coaching conversation differ for a secondary math team assessing more complex standards?
