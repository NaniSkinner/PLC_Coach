---
title: "Coaching Scenario: Deep Item Analysis to Inform Re-Teaching"
type: "coaching_scenario"
topics: ["item analysis", "re-teaching", "misconceptions", "data analysis", "critical question 2"]
critical_question: [2, 3]
grade_level: "high_school"
subject: "mathematics"
scenario_type: "Q2_item_analysis"
difficulty: "intermediate"
author: "AI PLC Coach"
publication_year: 2024
document_type: "coaching_scenario"
---

# Coaching Scenario: Deep Item Analysis to Inform Re-Teaching

## Context

David is a high school algebra teacher whose team just reviewed their first CFA results on quadratic equations. Overall proficiency was 62%, which seemed reasonable, but David feels like the team isn't digging deep enough into the data to really understand what students know and don't know. His team wants to move on to the next unit, but David thinks they need to analyze item-level data more carefully and do some targeted re-teaching first.

**Team Composition:**
- 4 Algebra 1 teachers
- School in year 2 of PLC implementation
- Team does basic data analysis but hasn't done deep item analysis
- Good relationships but task-focused (want to "get through" agendas)

**Current Situation:**
- Administered 15-item CFA on quadratic equations
- Looked at overall proficiency (62%) and called it "okay"
- Team ready to move to next unit (systems of equations)
- David concerned they're not learning from the data
- Team meeting later today

## The Coaching Conversation

### Turn 1: Teacher Opens

**David:** Our team just looked at our quadratic equations assessment data yesterday, and I'm frustrated. We looked at the overall proficiency rate - 62% - and basically everyone said "that's pretty good, let's move on." But I think we're missing a huge opportunity to actually learn from this data.

I looked at the item-level data on my own, and there are clear patterns. Like, 85% of students got question 3 right (solving by factoring), but only 38% got question 9 right (solving by completing the square), even though both questions assess solving quadratics. That's a massive difference! Doesn't that tell us something important about our instruction?

But when I tried to bring this up, people just said "some questions are harder" and wanted to move on. How do I get my team to dig deeper into data instead of just looking at surface-level numbers?

**Coach's Purpose:** Affirm his instinct to dig deeper, validate frustration, introduce value of item analysis, provide approach for engaging team.

**Coach Response:**

David, your instinct is exactly right. Looking only at overall proficiency is like a doctor taking your temperature, seeing it's 99 degrees, and saying "close enough to normal!" without investigating why you came to the office. The real insights are in the details - in this case, the item-level data.

That 85% vs. 38% comparison you made? That's goldmine information. It tells you students can factor but can't complete the square - which suggests they either didn't learn completing the square, or learned it but forgot it, or can't recognize when to use it. You'd teach each of those situations differently.

Here's the challenge: Item analysis takes more time and thought than overall proficiency checks. Your team is probably not resistant to deeper analysis - they just don't have a structure for doing it efficiently. It feels overwhelming, so they default to "good enough, move on."

Let me give you a protocol for item analysis that makes it manageable. Then I'll help you think about how to introduce it to your team.

Does that sound helpful?

---

### Turn 2: Teacher Eager for Structure

**David:** Yes, definitely. I think you're right that they're not resistant - they just don't know how to analyze item data efficiently. We spend so much time creating assessments that when we finally get results, everyone's tired and wants to just move forward.

What's the protocol? How do we analyze 15 items without spending hours on it?

**Coach's Purpose:** Provide efficient item analysis protocol, show how to identify patterns quickly, connect to instructional decisions.

**Coach Response:**

Here's a streamlined item analysis protocol that takes about 30 minutes for a 15-item assessment:

**Item Analysis Protocol (30 minutes):**

**Step 1: Sort Items by Difficulty (5 minutes)**
Create three categories based on % correct:
- **Strong Performance:** 75%+ correct (students have mastered)
- **Moderate Performance:** 50-74% correct (partial understanding)
- **Weak Performance:** Below 50% (significant gaps)

For your assessment:
- Strong: Questions 1, 3, 7, 8 (factoring, standard form identification)
- Moderate: Questions 2, 4, 5, 6, 10, 11, 12, 14 (mixed)
- Weak: Questions 9, 13, 15 (completing square, complex applications)

**Step 2: Look for Patterns Within Categories (10 minutes)**
Ask: "What do the weak items have in common?"

Your weak items:
- Q9: Solve by completing the square
- Q13: Choose appropriate method for given quadratic
- Q15: Word problem requiring quadratic model

Pattern: Students struggle with completing the square AND knowing when to use different methods.

**Step 3: Examine Wrong Answers (10 minutes)**
For weak items, look at which wrong answers students chose (if multiple choice) or what errors they made (if constructed response).

This reveals misconceptions vs. random guessing.

**Step 4: Make Instructional Decisions (5 minutes)**
For each pattern, decide:
- Skip (students have mastered) - move on
- Spiral Review (mostly solid, just need occasional practice) - add to warm-ups
- Re-teach (significant gaps) - dedicate class time

Based on your data, you'd re-teach completing the square and choosing methods.

Now, want to think through how you'd examine wrong answers for one of your weak items?

---

### Turn 3: Teacher Analyzes Specific Item

**David:** Yes! Let me look at question 9 - the completing the square one. The question was:

"Solve: x² + 6x + 2 = 0"

The right answer is x = -3 ± √7. Only 38% got it right.

Looking at wrong answers:
- 22% wrote x = -3 ± √11 (they added 9 to both sides but then wrote √(9+2) as √11 instead of √7)
- 18% wrote x = -3 ± 4 (they wrote √(9+2) as √9 + √2 = 3 + 1 = 4... wrong)
- 15% got to (x+3)² = 7 and stopped - didn't finish solving
- 7% had other various errors

What does this tell me about their understanding?

**Coach's Purpose:** Help him interpret error patterns, distinguish between conceptual errors and procedural errors, guide to targeted re-teaching.

**Coach Response:**

Excellent analysis, David! You've identified three distinct types of errors, each revealing different problems:

**Error Type 1: Procedural Mistake (22% with √11)**
These students:
- Know the completing square process
- Got to (x+3)² = 11 correctly
- Made arithmetic error (9+2 = 11 instead of 7)
- Proceeded correctly from their error

This is a **minor procedural error**, not conceptual gap.

**Error Type 2: Misconception (18% with ± 4)**
These students:
- Know the process
- Made **conceptual error**: √(a+b) ≠ √a + √b
- This is a fundamental misunderstanding of radicals

This needs targeted re-teaching of radical properties.

**Error Type 3: Incomplete Process (15% stopped)**
These students:
- Started process correctly
- Got to (x+3)² = 7
- Didn't know next step or ran out of time

This suggests they've memorized early steps but don't understand the full process or why it works.

**Instructional Implications:**

**For Group 1 (procedural errors):**
Minimal intervention - just more practice with emphasis on checking work

**For Group 2 (radical misconception):**
Re-teach radical properties with explicit instruction: √(a+b) ≠ √a + √b

**For Group 3 (incomplete process):**
Reteach with emphasis on why each step works and what the goal is

See how different errors require different responses? This is why item analysis matters - "only 38% got it right" doesn't tell you what to do next. But "here are three types of errors" gives you clear direction.

Does this help you see the value?

---

### Turn 4: Teacher Asks About Time Management

**David:** This is so helpful! I can see exactly how this would make my re-teaching more effective. But here's my concern: You said this takes 30 minutes. Looking at wrong answers like we just did for question 9 took us maybe 5-10 minutes just for ONE question.

If I have three weak items, that's 30-45 minutes just examining wrong answers. Plus categorizing items, looking for patterns... this could easily take a full 60-minute meeting or more.

My team is already going to say we don't have time. How do I convince them this is worth it when we're always behind on planning?

**Coach's Purpose:** Address time concern legitimately, show how item analysis saves time in long run, provide prioritization strategy.

**Coach Response:**

You're absolutely right that thorough item analysis takes time - probably 45-60 minutes for a 15-item assessment when you're first building the skill. I won't pretend otherwise. But let me reframe the time investment:

**Option A: Skip Item Analysis**
- Time spent: 10 minutes (glance at overall proficiency)
- Outcome: Move to next unit with 38% of students unable to complete the square
- Future cost: Re-teach later when gaps show up, or students fail end-of-unit test, or struggle all year

**Option B: Do Item Analysis**
- Time spent: 60 minutes (deep analysis)
- Outcome: Targeted re-teaching that addresses specific misconceptions
- Future benefit: Students master content, less remediation needed later, better foundation for future units

**Here's the question:** Is it worth 50 extra minutes now to prevent hours of re-teaching later?

**Also consider:** You're not analyzing 15 items deeply. You're analyzing the 3-4 items where students struggled. That's manageable.

**Strategy for Your Team:**
Propose this: "Let's invest one full meeting doing deep item analysis on this assessment. Let's see if it gives us useful information for re-teaching. If it feels like wasted time, we won't do it again. But if it helps us teach more effectively, it's worth the investment."

Frame it as an experiment, not a permanent mandate. My bet is that once your team experiences the insights from item analysis, they'll want to keep doing it.

Does that framing help?

---

### Turn 5: Teacher Asks About Presenting to Team

**David:** Yes, that helps. I think framing it as an experiment is smart - less threatening than "we must do this every time."

But I want to bring something concrete to my meeting today. If I said "let's try item analysis," they'd say "OK, what does that mean?" Can you help me prepare a specific example I could walk them through? Maybe I could show them the question 9 analysis we just did?

What would convince them this is worth the time?

**Coach's Purpose:** Help him prepare compelling presentation, provide template he can use, make it tangible and immediate.

**Coach Response:**

Great idea. Let me help you structure a compelling 5-minute opening that will get your team interested:

**Your Opening (5 minutes):**

**1. State the Problem (1 minute)**
"Yesterday we said 62% proficiency was 'pretty good.' But I looked deeper at the data and found something concerning. Question 3 (factoring) had 85% proficiency, but question 9 (completing square) had only 38%. If we move to systems of equations without addressing this, 62% of our students don't have the completing square tool available. That matters."

**2. Show Specific Example (2 minutes)**
"Let me show you what I found when I analyzed question 9's wrong answers. [Share the three error types you identified]. Notice these are THREE DIFFERENT problems requiring three different solutions. We can't just say 'reteach completing the square.' We need to address radical misconceptions for this group, incomplete processes for that group, and just give more practice to this group."

**3. Propose Experiment (1 minute)**
"I think if we spent 30-45 minutes doing this kind of analysis for our weak items, we'd plan much more effective re-teaching. I'd like to try it today - just this once - and see if it's worth the time. If it doesn't help, we don't do it again. What do you think?"

**4. If They Say Yes (1 minute)**
"Great. I actually prepared analysis for our three weakest items. Let me walk you through the protocol..."

**What Makes This Compelling:**
- Concrete example (not vague "we should analyze more")
- Shows immediate relevance (affects instruction this week)
- Low commitment (experiment, not mandate)
- You've done prep work (not asking them to start from scratch)

Want to practice presenting this, or do you feel ready?

---

### Turn 6: Teacher Feels Ready but Asks About Re-Teaching

**David:** I think I can present that. But let's say they agree and we do the analysis. Then what? We identify that students have three different types of errors on completing the square. How do I actually RE-TEACH this?

Do I whole-class reteach? Do I pull small groups? Do I make videos? And how much time do I spend on this when I've got to keep moving forward in the curriculum?

**Coach's Purpose:** Connect item analysis to differentiated re-teaching strategies, provide practical options, address pacing concern.

**Coach Response:**

Perfect question - item analysis is only valuable if it informs action. Let me give you a framework for translating analysis into re-teaching:

**Re-Teaching Decision Matrix:**

**If 75%+ of students got it RIGHT:**
- Action: Spiral review only (include in warm-ups, homework)
- Time: 5 minutes per week
- Method: Whole class, embedded in ongoing work

**If 50-75% got it RIGHT:**
- Action: Brief whole-class re-teaching
- Time: 10-15 minutes
- Method: Mini-lesson highlighting key points, guided practice

**If LESS than 50% got it RIGHT (your completing square situation):**
- Action: Differentiated re-teaching based on error types
- Time: 20-30 minutes
- Method: Combination of whole-class and targeted small-group

**For Your Question 9:**

**Day 1: 15-Minute Whole-Class Mini-Lesson**
Address the most common misconception (radical properties):
- Teach: √(a+b) ≠ √a + √b
- Show examples and non-examples
- Practice 3-4 problems together

**Day 2: 15-Minute Differentiated Practice**
While students work on problems:
- Pull small group with students who stopped at (x+3)² = 7 and explicitly teach finishing steps
- Other students practice independently
- Circulate to students who made procedural errors

**Day 3: Move Forward with Spiraled Practice**
- Start systems of equations unit
- Include 1-2 completing square problems in daily warm-ups for next 2 weeks

**Total time investment:** 30 minutes re-teaching, then spiraled practice

**About Pacing:**
You're not stopping progress completely. You're investing 2-3 days to shore up a critical skill, then moving forward with spiraled review. That's way more effective than moving forward with huge gaps.

Does that feel manageable?

---

### Turn 7: Teacher Connects to Intervention

**David:** Yes, that feels manageable. But I'm realizing - what about the students who STILL don't get it after the re-teaching? I'll have some students who need even more support. Is that where intervention comes in?

And also, what about the students who DID master completing the square (the 38% who got it right)? Do they have to sit through all this re-teaching?

**Coach's Purpose:** Connect Question 2 (assessment) to Questions 3 and 4 (intervention and extension), show how item analysis drives systematic response.

**Coach Response:**

David, you've just connected the dots between all four critical questions! Yes - item analysis naturally leads to differentiated response:

**Question 1:** What should they learn? (Solving quadratics by multiple methods)

**Question 2:** How will we know they learned it? (Common assessment with item analysis)

**Question 3:** What when they don't learn it?
- Whole-class re-teaching for widespread gaps (completing the square)
- Tier 2 intervention for students who don't respond to re-teaching
- In your case: 5-8 students who still can't complete square after mini-lessons would need small-group intervention

**Question 4:** What when they already know it?
- The 38% who mastered completing square (plus students who master it during re-teaching)
- Give them extension: "Solve this quadratic by ALL THREE methods (factoring, completing square, quadratic formula). When would you choose each method and why?"
- This deepens their understanding without just giving them more of the same

**Practical Implementation:**

**During your 15-minute mini-lessons:**
Post extension problems for students who already mastered it. They work independently while you teach the others.

**For intervention:**
After re-teaching, identify the 5-8 students who still struggle. These students need systematic Tier 2 intervention:
- 20 minutes, 3x/week, small group
- Explicit instruction on completing square
- More practice opportunities
- Progress monitoring every week

See how item analysis is the engine that drives your whole response system? Without knowing WHAT students don't understand, you can't target intervention or extension effectively.

This making sense?

---

### Turn 8: Teacher Commits to Action and Plans Follow-Up

**David:** Yes! This all makes sense. OK, here's what I'm going to do:

**Today's Meeting:**
1. Open with my 5-minute pitch about why item analysis matters
2. Walk team through the protocol for our three weak items
3. Make decisions together about which items need whole-class re-teaching
4. Divide up the planning - maybe each person takes one re-teaching topic

**This Week:**
1. Implement the mini-lessons we plan
2. Identify students needing Tier 2 intervention based on who responds to re-teaching
3. Give extension problems to students who already mastered content

**Ongoing:**
1. Propose we do item analysis for all future CFAs (if today goes well)
2. Build intervention time into our schedule for students with persistent gaps

Is there anything I'm missing? And would you be willing to check in with me after today's meeting to hear how the item analysis discussion went?

**Coach's Purpose:** Affirm his comprehensive plan, offer refinement, confirm ongoing support.

**Coach Response:**

David, your plan is excellent and comprehensive. You're thinking short-term (today's meeting), medium-term (this week's instruction), and long-term (building sustainable practices). That's leadership.

One small addition to your plan:

**After Re-Teaching (Next Week):**
Give a quick check (3-5 problems) on completing the square to see if re-teaching worked. This:
- Shows you whether intervention is needed
- Proves to your team that the time was well-spent
- Closes the loop on the data-instruction-data cycle

This makes the whole process feel complete.

Also, for your meeting today, bring the data already organized. Don't ask your team to categorize items during the meeting - that's prep work you do in advance. Just show them: "Here are our weak items. Here's what I noticed about error patterns. Let's discuss."

The more prepared you are, the easier it is for them to say yes.

I'd absolutely love to hear how today goes. Specifically, I'm curious:
- Did your team engage with item analysis or resist?
- What did they learn from looking at wrong answers?
- Did they commit to re-teaching based on the data?

You've got this, David. Your students are lucky to have a teacher who looks beyond surface-level data to really understand their learning.

---

## Coaching Analysis

### Effective Coaching Moves Used

**1. Validated Teacher's Instinct**
Immediately affirmed that David's frustration with surface-level analysis was justified and his instinct to dig deeper was correct.

**2. Provided Efficient Protocol**
Gave streamlined item analysis process that addresses team's time concerns while still being thorough.

**3. Used Concrete Examples**
Worked through David's actual assessment item (question 9) to make abstract concepts tangible.

**4. Distinguished Error Types**
Helped David see difference between procedural errors, conceptual misconceptions, and incomplete processes - each requiring different responses.

**5. Connected to Action**
Repeatedly linked analysis to instructional decisions, showing that data work leads to better teaching.

**6. Addressed Practical Obstacles**
Tackled time concerns, team resistance, and implementation challenges head-on with realistic solutions.

**7. Connected All Four Questions**
Showed how item analysis (Q2) naturally leads to intervention (Q3) and extension (Q4), all grounded in essential standards (Q1).

### Key PLC Concepts Demonstrated

- **Item-Level Data Analysis:** Going beyond overall proficiency to understand specific strengths and gaps
- **Error Pattern Analysis:** Examining wrong answers to identify misconceptions
- **Targeted Re-Teaching:** Using data to inform differentiated instruction
- **Time as Variable, Learning as Constant:** Investing time in re-teaching rather than moving forward with gaps
- **Connection to Intervention:** Item analysis identifies students needing Tier 2 support
- **Extension for Proficient Students:** Students who mastered content receive deeper challenges
- **Collaborative Decision-Making:** Team analyzes data together and plans response collaboratively

### Likely Outcomes

If David implements this coaching advice:
- **Immediate:** Team will engage in deeper item analysis and plan targeted re-teaching
- **Short-term:** Students will demonstrate improved understanding of completing the square
- **Medium-term:** Team will adopt item analysis as regular practice
- **Long-term:** Instruction becomes more responsive to specific student needs; student achievement improves

### Potential Challenges Ahead

1. **Team Resistance:** Some teachers may still resist time investment despite David's framing
2. **Inconsistent Implementation:** Teachers may plan re-teaching but not follow through
3. **Surface-Level Analysis:** Team might look at wrong answers without identifying patterns
4. **Moving Too Fast:** Pressure to cover content may override commitment to re-teaching

The coach has positioned David well by providing both technical tools (protocol) and interpersonal strategies (how to present to team).

## Reflection Questions for Coaches

1. How did the coach help David move from general frustration to specific, actionable plan?

2. What evidence suggests the coach was building David's capacity to lead his team, not just solve one data problem?

3. How did the coach balance acknowledging time constraints with pushing for deeper analysis?

4. What follow-up coaching might David need after implementing his re-teaching plan?

5. How might this coaching conversation differ for an elementary team analyzing reading assessment data?
