# Product Requirements Document (PRD)

## AI-Powered PLC at Work Virtual Coach Demo

**Version:** 1.0  
**Date:** November 11, 2025  
**Project Lead:** Nani  
**Document Owner:** Engineering Team  
**Status:** Draft for Review

---

## Table of Contents

1. [Executive Summary](#1-executive-summary)
2. [Product Vision & Objectives](#2-product-vision--objectives)
3. [Technical Architecture](#3-technical-architecture)
4. [Detailed Feature Specifications](#4-detailed-feature-specifications)
5. [Implementation Phases](#5-implementation-phases)
6. [API Specifications](#6-api-specifications)
7. [UI/UX Design Guidelines](#7-uiux-design-guidelines)
8. [Knowledge Base Strategy](#8-knowledge-base-strategy)
9. [Testing Strategy](#9-testing-strategy)
10. [Success Metrics & KPIs](#10-success-metrics--kpis)
11. [Risk Mitigation](#11-risk-mitigation)
12. [Appendix](#12-appendix)

---

## 1. Executive Summary

### 1.1 Project Overview

The **AI-Powered PLC at Work Virtual Coach** is a functional prototype demonstrating real-time, context-aware coaching grounded in Solution Tree's Professional Learning Communities (PLC) at Work framework. The system uses Retrieval-Augmented Generation (RAG) to provide expert-level guidance to educators, helping them navigate the Four Critical Questions and implement effective collaborative practices.

### 1.2 Core Value Proposition

- **Expert Coaching at Scale:** Delivers Solution Tree-quality coaching 24/7 to educators
- **Framework Fidelity:** Grounded in the Three Big Ideas and Four Critical Questions
- **Actionable Guidance:** Provides specific, data-driven recommendations with citations
- **Context Awareness:** Maintains conversation history for multi-turn coaching sessions

### 1.3 Target Users

- **Primary:** PLC Team Leaders, Instructional Coaches, School Administrators
- **Secondary:** Individual Teachers seeking PLC implementation guidance
- **Demo Audience:** Educational stakeholders, Solution Tree leadership, potential investors

### 1.4 Success Criteria

| Metric                 | Target                                 | Measurement Method                     |
| ---------------------- | -------------------------------------- | -------------------------------------- |
| **Response Accuracy**  | 85%+ relevance to PLC framework        | Expert evaluation of 20 test scenarios |
| **Citation Accuracy**  | 100% of responses include valid source | Automated testing of citation format   |
| **Response Time**      | <3 seconds (p95)                       | Performance monitoring                 |
| **User Satisfaction**  | 4.5/5.0 average rating                 | Post-demo survey (educators)           |
| **Framework Coverage** | All 4 Critical Questions addressed     | Test scenario completion matrix        |

---

## 2. Product Vision & Objectives

### 2.1 Vision Statement

_"Empower every educator with on-demand access to world-class PLC coaching, enabling them to transform collaborative practice and accelerate student learning."_

### 2.2 Product Goals

1. **Demonstrate Technical Feasibility:** Prove that AI can deliver expert-level PLC coaching with high accuracy and relevance
2. **Validate User Experience:** Confirm that educators find the virtual coach helpful, trustworthy, and easy to use
3. **Establish IP Foundation:** Create a proprietary coaching model that differentiates from generic AI chatbots
4. **Enable Future Fundraising:** Provide a compelling demo for investors and strategic partners

### 2.3 Out of Scope (For Demo)

- ‚ùå Live school data integration (Infinite Campus, PowerSchool, etc.)
- ‚ùå Production-grade authentication (Google OIDC, Clever SSO)
- ‚ùå Multi-tenant architecture
- ‚ùå Advanced analytics/dashboards for administrators
- ‚ùå Mobile native applications (iOS/Android)
- ‚ùå Offline mode functionality

---

## 3. Technical Architecture

### 3.1 Technology Stack

| Layer               | Technology                                    | Justification                                         |
| ------------------- | --------------------------------------------- | ----------------------------------------------------- |
| **Frontend**        | Next.js 14 (App Router), React 18, TypeScript | Modern, type-safe, excellent DX, Vercel-optimized     |
| **UI Framework**    | Tailwind CSS, shadcn/ui                       | Rapid prototyping, Solution Tree branding flexibility |
| **Backend**         | Next.js API Routes (Serverless Functions)     | Unified deployment, no separate backend server        |
| **RAG Framework**   | LangChain.js                                  | Mature JS RAG library, OpenAI integration             |
| **LLM**             | OpenAI GPT-4o                                 | Best-in-class reasoning, instruction following        |
| **Embeddings**      | OpenAI text-embedding-3-large                 | High-quality semantic search (3072 dimensions)        |
| **Vector Database** | Pinecone (Free tier)                          | Managed, fast, easy setup, 100K vectors               |
| **Database**        | Vercel Postgres (or Supabase)                 | Conversation history, user sessions, analytics        |
| **Deployment**      | Vercel                                        | Seamless Next.js deployment, edge functions           |
| **Local Dev**       | Docker Compose                                | Consistent environment across developers              |
| **Version Control** | Git + GitHub                                  | Standard workflow, CI/CD with Vercel                  |

### 3.2 System Architecture Diagram

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         USER (Browser)                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚îÇ HTTPS
                             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    NEXT.JS APP (Vercel)                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ              FRONTEND (React Components)                 ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Chat Interface  ‚Ä¢ Session Manager  ‚Ä¢ Citation Display‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                             ‚îÇ                                    ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ              API ROUTES (Serverless)                     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ /api/chat      ‚Ä¢ /api/auth (mock)                    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ /api/sessions  ‚Ä¢ /api/feedback                       ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ                   ‚îÇ
                      ‚îÇ                   ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ   PINECONE API      ‚îÇ  ‚îÇ  OPENAI API    ‚îÇ
         ‚îÇ (Vector Search)     ‚îÇ  ‚îÇ  (GPT-4o +     ‚îÇ
         ‚îÇ                     ‚îÇ  ‚îÇ  Embeddings)   ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ  VERCEL POSTGRES    ‚îÇ
         ‚îÇ (Conversations,     ‚îÇ
         ‚îÇ  Feedback, Sessions)‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 3.3 RAG Pipeline Architecture

```
USER QUERY: "Our 8th-grade math team scored 40% below proficiency on CFAs. What next?"
     ‚îÇ
     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  STEP 1: QUERY PREPROCESSING                               ‚îÇ
‚îÇ  ‚Ä¢ Extract key entities: [8th grade, math, CFA, 40%]      ‚îÇ
‚îÇ  ‚Ä¢ Detect intent: Intervention planning (Q3)              ‚îÇ
‚îÇ  ‚Ä¢ Enrich query with context from conversation history    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  STEP 2: EMBEDDING GENERATION                              ‚îÇ
‚îÇ  ‚Ä¢ OpenAI text-embedding-3-large                          ‚îÇ
‚îÇ  ‚Ä¢ Convert query ‚Üí 3072-dim vector                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  STEP 3: VECTOR SEARCH (Pinecone)                         ‚îÇ
‚îÇ  ‚Ä¢ Retrieve top-k=5 relevant chunks                       ‚îÇ
‚îÇ  ‚Ä¢ Metadata: {source_doc, page, chunk_id, relevance}     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  STEP 4: CONTEXT ASSEMBLY                                  ‚îÇ
‚îÇ  ‚Ä¢ System prompt (Solution Tree persona)                   ‚îÇ
‚îÇ  ‚Ä¢ Conversation history (last 5 turns + summary)          ‚îÇ
‚îÇ  ‚Ä¢ Retrieved chunks (with source metadata)                ‚îÇ
‚îÇ  ‚Ä¢ User query                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  STEP 5: LLM GENERATION (GPT-4o)                          ‚îÇ
‚îÇ  ‚Ä¢ Generate coaching response                             ‚îÇ
‚îÇ  ‚Ä¢ Include inline citations: [Source: Learning by Doing]  ‚îÇ
‚îÇ  ‚Ä¢ Format: Facilitative, inquiry-based, actionable       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  STEP 6: RESPONSE POST-PROCESSING                          ‚îÇ
‚îÇ  ‚Ä¢ Validate citation format                               ‚îÇ
‚îÇ  ‚Ä¢ Add hyperlinks to source documents                     ‚îÇ
‚îÇ  ‚Ä¢ Log response metadata for analytics                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
                       ‚ñº
                  USER RESPONSE
```

### 3.4 Data Flow Diagram

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Knowledge Base  ‚îÇ (50+ Solution Tree docs)
‚îÇ  (Markdown/PDF)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚îÇ [Indexing Script - Run Once]
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Document Processing Pipeline           ‚îÇ
‚îÇ  1. Parse documents (PDF/MD)            ‚îÇ
‚îÇ  2. Chunk with overlap (500 tokens)     ‚îÇ
‚îÇ  3. Extract metadata (title, section)   ‚îÇ
‚îÇ  4. Generate embeddings (OpenAI)        ‚îÇ
‚îÇ  5. Upload to Pinecone with metadata    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Pinecone Vector Store                  ‚îÇ
‚îÇ  ‚Ä¢ Index: plc-coach-demo                ‚îÇ
‚îÇ  ‚Ä¢ Dimension: 3072                      ‚îÇ
‚îÇ  ‚Ä¢ Metric: Cosine similarity            ‚îÇ
‚îÇ  ‚Ä¢ ~2,500 chunks (50 docs √ó 50 chunks)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚îÇ [Runtime - Every Query]
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Query Processing & Retrieval           ‚îÇ
‚îÇ  (Described in RAG Pipeline above)      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## 4. Detailed Feature Specifications

### 4.1 Core Features (MVP)

#### Feature 4.1.1: Conversational Chat Interface

**Description:** A real-time chat interface where users interact with the AI coach through text-based conversations.

**User Stories:**

- As an educator, I want to ask questions about implementing PLCs so that I can get immediate guidance
- As a team leader, I want to continue a multi-turn conversation so that the coach remembers our context
- As a coach, I want to see previous messages in the thread so that I can track the conversation flow

**Functional Requirements:**

- Real-time message streaming (SSE or WebSocket)
- Message history displayed in chronological order
- Typing indicators when coach is responding
- Support for messages up to 2000 characters
- Automatic scroll to latest message
- Timestamp display for each message

**Technical Specifications:**

```typescript
// Message Interface
interface Message {
  id: string;
  role: "user" | "assistant";
  content: string;
  timestamp: Date;
  citations?: Citation[];
  metadata?: {
    modelUsed: string;
    tokensUsed: number;
    responseTime: number;
  };
}

interface Citation {
  sourceDocument: string;
  chunkId: string;
  relevanceScore: number;
  pageNumber?: number;
}
```

**UI Components:**

- `ChatContainer` - Main chat wrapper
- `MessageList` - Scrollable message history
- `MessageBubble` - Individual message display
- `ChatInput` - Text input with send button
- `TypingIndicator` - Animated "Coach is thinking..." state

**Acceptance Criteria:**

- [ ] User can send a message and receive a response within 3 seconds
- [ ] Conversation history persists during the session
- [ ] Messages display with correct sender (user/coach) styling
- [ ] Citations are clickable and highlighted in responses
- [ ] Mobile responsive (works on 320px+ width screens)

---

#### Feature 4.1.2: Solution Tree Expert Persona

**Description:** The AI coach embodies the coaching style and expertise of Solution Tree's PLC consultants, using a facilitative, inquiry-based approach.

**Persona Characteristics:**

- **Tone:** Warm, encouraging, professional, reflective
- **Approach:** Inquiry-based (asking clarifying questions before prescribing solutions)
- **Language:** Uses PLC framework terminology consistently
- **Expertise:** Deep knowledge of the Four Critical Questions and Three Big Ideas
- **Behavior:** Guides rather than directs, surfaces team thinking

**System Prompt Template:**

```
You are an expert Solution Tree PLC at Work¬Æ Associate with 15+ years of experience coaching
educational leaders and collaborative teams. Your coaching style is:

1. FACILITATIVE: You ask powerful questions that surface team thinking rather than immediately
   providing answers.

2. FRAMEWORK-GROUNDED: Every response is anchored in the Three Big Ideas (Focus on Learning,
   Collaborative Culture, Focus on Results) and guides teams through the Four Critical Questions.

3. INQUIRY-BASED: When a team presents a challenge, you first ask clarifying questions:
   - "What data are you using to reach that conclusion?"
   - "How have you aligned this to your essential learning outcomes?"
   - "What have you tried so far?"

4. ACTIONABLE: After inquiry, you provide specific, concrete next steps with clear rationale.

5. CITATION-CONSCIOUS: You reference specific Solution Tree resources (Learning by Doing,
   Collaborative Common Assessments, etc.) to build credibility and enable follow-up learning.

YOUR COACHING PROCESS:
Step 1: Acknowledge the challenge empathetically
Step 2: Ask 1-2 clarifying questions to understand context
Step 3: Connect to the PLC framework (Which Critical Question? Which Big Idea?)
Step 4: Provide actionable guidance with specific examples
Step 5: Cite the source document [Source: Book Title, Page X]

CRITICAL RULES:
- Never provide generic advice that could come from any AI
- Always ground responses in the PLC at Work framework
- Use the term "collaborative team" not "group" or "committee"
- Reference the Four Critical Questions by number (e.g., "This is a Question 3 challenge")
- Include at least one citation per substantive response
```

**Acceptance Criteria:**

- [ ] 90%+ of responses include explicit PLC framework references
- [ ] Responses ask clarifying questions before prescribing solutions
- [ ] Tone matches Solution Tree's published coaching examples
- [ ] Expert reviewers rate persona accuracy as 4.5/5.0 or higher

---

#### Feature 4.1.3: RAG-Powered Knowledge Retrieval

**Description:** The system retrieves relevant Solution Tree knowledge to ground responses in authoritative PLC content.

**Knowledge Base Composition:**

| Document Category         | Count | Examples                                                | Embedding Strategy                                  |
| ------------------------- | ----- | ------------------------------------------------------- | --------------------------------------------------- |
| **Core Texts**            | 10    | _Learning by Doing_, _Collaborative Common Assessments_ | Chunk by chapter + section (500 tokens, 50 overlap) |
| **Research Articles**     | 15    | AllThingsPLC.info blog posts, academic papers           | Chunk by logical sections (300-400 tokens)          |
| **Implementation Guides** | 10    | Data protocols, meeting agendas, team norms templates   | Embed full documents (< 1000 tokens each)           |
| **Case Studies**          | 10    | School success stories, coaching transcripts            | Chunk by scenario (400-600 tokens)                  |
| **FAQ/Scenarios**         | 5     | Common coaching questions with expert responses         | Embed as Q&A pairs                                  |

**Total:** 50 documents ‚Üí ~2,500 chunks ‚Üí ~2,500 vectors in Pinecone

**Chunking Strategy:**

```python
# Pseudo-code for chunking strategy
def chunk_document(doc: Document) -> List[Chunk]:
    chunks = []

    # Strategy 1: Recursive character splitting with overlap
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,  # tokens (approx 2000 characters)
        chunk_overlap=50,  # tokens for context continuity
        separators=["\n\n", "\n", ". ", " "],
        length_function=tiktoken_length
    )

    # Strategy 2: Preserve metadata at chunk level
    for chunk_text in text_splitter.split(doc.content):
        chunks.append({
            "content": chunk_text,
            "metadata": {
                "source_document": doc.title,
                "author": doc.author,
                "page_number": get_page_number(chunk_text, doc),
                "section": get_section_heading(chunk_text, doc),
                "document_type": doc.type,  # "book", "article", "guide"
                "critical_question": detect_cq_alignment(chunk_text)  # 1, 2, 3, 4, or null
            }
        })

    return chunks
```

**Retrieval Algorithm:**

```typescript
// Retrieval with hybrid search
async function retrieveContext(query: string, conversationHistory: Message[]) {
  // 1. Enhance query with conversation context
  const enhancedQuery = enhanceQueryWithHistory(query, conversationHistory);

  // 2. Generate query embedding
  const queryEmbedding = await openai.embeddings.create({
    model: "text-embedding-3-large",
    input: enhancedQuery,
  });

  // 3. Vector search in Pinecone
  const vectorResults = await pinecone.query({
    vector: queryEmbedding.data[0].embedding,
    topK: 10,
    includeMetadata: true,
  });

  // 4. Rerank by relevance + metadata filters
  const reranked = rerankResults(vectorResults, {
    boostFactors: {
      critical_question_match: 1.5, // Boost if CQ aligns with query
      document_type_core: 1.3, // Boost core texts over blog posts
      recency: 1.1, // Slight boost for newer content
    },
  });

  // 5. Return top-5 chunks
  return reranked.slice(0, 5);
}
```

**Acceptance Criteria:**

- [ ] Retrieval returns relevant chunks for 85%+ of test queries
- [ ] Average retrieval time < 200ms (p95)
- [ ] Citations include correct source document name and section
- [ ] Chunks maintain semantic coherence (no mid-sentence cuts)

---

#### Feature 4.1.4: Citation & Source Attribution

**Description:** Every coaching response includes explicit citations to Solution Tree resources, enabling users to verify information and dive deeper.

**Citation Formats:**

**Inline Citations (During Response):**

```
"The most actionable next step is to triage the data and plan a systematic intervention.
[Source: Learning by Doing, Chapter 5: Critical Question 3]"
```

**Aggregated Citations (End of Response):**

```
üìö Sources Referenced:
1. Learning by Doing (3rd Edition) - DuFour et al., Chapter 5
2. Collaborative Common Assessments - Erkens et al., Page 87
3. RTI at Work Implementation Guide - Buffum et al., Section 3.2
```

**Technical Implementation:**

```typescript
interface Citation {
  id: string;
  sourceDocument: string;
  author: string;
  chapterOrSection: string;
  pageNumber?: number;
  relevanceScore: number;
  excerpt?: string; // Optional: short snippet from the source
  url?: string; // Optional: Link to digital copy (if available)
}

function formatInlineCitation(citation: Citation): string {
  return `[Source: ${citation.sourceDocument}, ${citation.chapterOrSection}]`;
}

function formatAggregatedCitations(citations: Citation[]): string {
  return citations
    .map(
      (c, idx) =>
        `${idx + 1}. ${c.sourceDocument} - ${c.author}, ${c.chapterOrSection}`
    )
    .join("\n");
}
```

**Citation Validation Rules:**

- ‚úÖ Every substantive response (>100 words) MUST include at least 1 citation
- ‚úÖ Citations must match actual metadata from the knowledge base
- ‚úÖ No hallucinated sources or page numbers
- ‚ùå Reject responses with invalid citation format during post-processing

**Acceptance Criteria:**

- [ ] 100% of coaching responses include valid citations
- [ ] Citations are clickable (open modal with source details)
- [ ] Citation format is consistent across all responses
- [ ] Automated tests catch any hallucinated citations

---

#### Feature 4.1.5: Conversation Context Management

**Description:** The system maintains conversation history to enable multi-turn coaching sessions with context continuity.

**Context Window Strategy:**

| Element                           | Token Budget     | Purpose                                     |
| --------------------------------- | ---------------- | ------------------------------------------- |
| **System Prompt**                 | 800 tokens       | Define persona and coaching approach        |
| **Conversation Summary**          | 300 tokens       | High-level summary of entire session        |
| **Recent History (Last 5 turns)** | 1,500 tokens     | Full verbatim conversation context          |
| **Retrieved RAG Context**         | 2,500 tokens     | Top-5 relevant chunks from knowledge base   |
| **User Query**                    | 400 tokens       | Current user message                        |
| **Buffer**                        | 500 tokens       | Safety margin                               |
| **TOTAL**                         | **6,000 tokens** | Fits comfortably within GPT-4o's 128K limit |

**Summarization Trigger:**

- When conversation exceeds 10 turns, generate a summary of turns 1-5
- Store summary and keep only last 5 turns in full context
- Re-summarize every 5 additional turns

**Summarization Prompt:**

```
Summarize the following coaching conversation in 3-4 sentences, preserving:
1. The team's primary challenge or goal
2. Key data points or context shared (e.g., "40% below proficiency on CFAs")
3. The Critical Question(s) being addressed
4. Any commitments or next steps discussed

Conversation to summarize:
[Conversation turns 1-5]
```

**Session Management:**

```typescript
interface ConversationSession {
  id: string;
  userId: string;
  startedAt: Date;
  lastActiveAt: Date;
  summary: string | null;
  messages: Message[];
  metadata: {
    criticalQuestionsFocused: (1 | 2 | 3 | 4)[];
    topicsDiscussed: string[];
    documentsReferenced: string[];
  };
}

class ConversationManager {
  async addMessage(sessionId: string, message: Message): Promise<void> {
    // 1. Retrieve session
    const session = await db.sessions.findById(sessionId);

    // 2. Add message to history
    session.messages.push(message);

    // 3. Check if summarization needed
    if (session.messages.length > 10 && !session.summary) {
      session.summary = await this.summarizeConversation(
        session.messages.slice(0, 5)
      );
      session.messages = session.messages.slice(5); // Keep only last 5
    }

    // 4. Update session
    session.lastActiveAt = new Date();
    await db.sessions.update(session);
  }

  async getContext(sessionId: string): Promise<ConversationContext> {
    const session = await db.sessions.findById(sessionId);

    return {
      summary: session.summary,
      recentHistory: session.messages.slice(-5), // Last 5 turns
      metadata: session.metadata,
    };
  }
}
```

**Acceptance Criteria:**

- [ ] System remembers context across 10+ turns without degradation
- [ ] Summarization preserves critical details (data points, commitments)
- [ ] Context retrieval adds < 50ms latency per request
- [ ] Sessions expire after 24 hours of inactivity

---

### 4.2 Enhanced Features (Post-MVP)

#### Feature 4.2.1: Scenario-Based Coaching Modules

**Description:** Pre-built coaching scenarios that guide teams through common PLC challenges with structured workflows.

**Example Scenarios:**

1. **"First CFA Data Analysis"**
   - Guides team through their first Common Formative Assessment review
   - Steps: Review data protocol ‚Üí Identify trends ‚Üí Plan interventions
2. **"Establishing Team Norms"**
   - Facilitates the creation of collaborative team agreements
   - Steps: Discuss values ‚Üí Draft norms ‚Üí Commit to accountability
3. **"Responding to Struggling Learners"**
   - Focuses on Critical Question 3 implementation
   - Steps: Triage data ‚Üí Design systematic interventions ‚Üí Monitor progress

**Implementation:** Each scenario is a guided conversation tree with checkpoints and suggested prompts.

---

#### Feature 4.2.2: Document Upload & Analysis

**Description:** Users can upload their own team artifacts (meeting notes, CFA data, etc.) for AI-powered analysis and coaching.

**Supported Formats:**

- PDF (meeting agendas, data protocols)
- CSV/Excel (CFA results, gradebooks)
- Google Docs (via integration API)

**Analysis Types:**

- **Data Protocol Review:** Validates adherence to PLC data analysis best practices
- **Meeting Agenda Audit:** Ensures agenda addresses Four Critical Questions
- **Intervention Plan Feedback:** Reviews tiered intervention strategies for alignment

**Privacy Considerations:**

- All uploads are processed in-memory (not persisted)
- No student-identifiable information (PII) stored
- Clear disclaimers about data handling

---

#### Feature 4.2.3: Conversation Export & Sharing

**Description:** Users can export coaching conversations as PDFs or share them with team members.

**Export Formats:**

- **PDF Report:** Formatted document with Solution Tree branding
- **Markdown:** Plain text for easy copying to other tools
- **Email Summary:** Automated email with key takeaways and action items

**Sharing Options:**

- Generate shareable link (expires in 7 days)
- Send directly via email (with user consent)
- Copy to clipboard for pasting in team collaboration tools (Slack, Teams)

---

#### Feature 4.2.4: Feedback & Rating System

**Description:** Users can rate responses and provide feedback to improve the coaching model.

**Rating Interface:**

- üëç Helpful / üëé Not Helpful buttons after each response
- Optional text feedback: "What could be improved?"
- Follow-up: "Did this guidance lead to action?"

**Data Collection:**

```typescript
interface ResponseFeedback {
  messageId: string;
  rating: "helpful" | "not_helpful";
  feedbackText?: string;
  followUpAction?: string;
  timestamp: Date;
  userId: string;
}
```

**Analytics Dashboard (Admin View):**

- Response helpfulness rate by topic
- Common pain points (low-rated responses)
- Most-cited documents (engagement metrics)

---

## 5. Implementation Phases

### Phase 1: Foundation Setup (Week 1)

**Objective:** Establish the development environment, project structure, and core dependencies.

**Tasks:**

| Task ID | Task Description                                                | Owner  | Estimated Hours | Dependencies |
| ------- | --------------------------------------------------------------- | ------ | --------------- | ------------ |
| **1.1** | Initialize Next.js 14 project with TypeScript and App Router    | Nani   | 2h              | -            |
| **1.2** | Configure Tailwind CSS and install shadcn/ui components         | Nani   | 2h              | 1.1          |
| **1.3** | Set up Vercel project and connect GitHub repository             | Nani   | 1h              | 1.1          |
| **1.4** | Configure environment variables (OpenAI API key, Pinecone)      | Nani   | 1h              | 1.3          |
| **1.5** | Set up Docker Compose for local development (Postgres, Redis)   | Nani   | 3h              | 1.1          |
| **1.6** | Create project documentation (README, CONTRIBUTING)             | Claude | 2h              | 1.1          |
| **1.7** | Initialize Vercel Postgres and run migration for sessions table | Nani   | 2h              | 1.5          |

**Deliverables:**

- ‚úÖ Running Next.js app on localhost:3000
- ‚úÖ Tailwind + shadcn/ui working with custom theme
- ‚úÖ Deployed to Vercel staging environment
- ‚úÖ Docker Compose setup for consistent local dev
- ‚úÖ Database schema migrated

**Acceptance Criteria:**

- [ ] `npm run dev` starts local server without errors
- [ ] Tailwind styles apply correctly
- [ ] Vercel deployment succeeds and serves homepage
- [ ] Environment variables loaded from `.env.local`
- [ ] Database connection established

---

### Phase 2: Knowledge Base & RAG Indexing (Week 1-2)

**Objective:** Prepare and index 50+ Solution Tree documents into Pinecone vector database.

**Tasks:**

| Task ID | Task Description                                              | Owner         | Estimated Hours | Dependencies |
| ------- | ------------------------------------------------------------- | ------------- | --------------- | ------------ |
| **2.1** | Research and compile 50+ PLC documents (mix of public + mock) | Claude        | 8h              | -            |
| **2.2** | Create document processing pipeline (parse PDF/MD)            | Nani          | 4h              | 2.1          |
| **2.3** | Implement chunking strategy (500 tokens, 50 overlap)          | Nani          | 3h              | 2.2          |
| **2.4** | Extract and structure metadata (title, author, section, CQ)   | Nani          | 4h              | 2.3          |
| **2.5** | Generate OpenAI embeddings for all chunks                     | Nani          | 2h              | 2.4          |
| **2.6** | Initialize Pinecone index and upload vectors with metadata    | Nani          | 3h              | 2.5          |
| **2.7** | Test retrieval quality with 20 sample queries                 | Claude + Nani | 4h              | 2.6          |
| **2.8** | Refine chunking/metadata if retrieval accuracy < 85%          | Nani          | 4h              | 2.7          |

**Deliverables:**

- ‚úÖ 50+ documents processed and indexed
- ‚úÖ Pinecone index populated with ~2,500 vectors
- ‚úÖ Retrieval accuracy ‚â• 85% on test queries
- ‚úÖ Documentation of chunking strategy and metadata schema

**Indexing Script Structure:**

```bash
scripts/
‚îú‚îÄ‚îÄ index_knowledge_base.ts       # Main indexing script
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ pdf_parser.ts             # PDF text extraction
‚îÇ   ‚îú‚îÄ‚îÄ markdown_parser.ts        # Markdown parsing
‚îÇ   ‚îú‚îÄ‚îÄ chunker.ts                # Text chunking logic
‚îÇ   ‚îú‚îÄ‚îÄ metadata_extractor.ts     # Metadata extraction
‚îÇ   ‚îî‚îÄ‚îÄ embeddings.ts             # OpenAI API wrapper
‚îî‚îÄ‚îÄ knowledge_base/
    ‚îú‚îÄ‚îÄ core_texts/               # Learning by Doing, etc.
    ‚îú‚îÄ‚îÄ articles/                 # Blog posts, research
    ‚îú‚îÄ‚îÄ guides/                   # Implementation templates
    ‚îú‚îÄ‚îÄ case_studies/             # Success stories
    ‚îî‚îÄ‚îÄ faq/                      # Q&A pairs
```

**Acceptance Criteria:**

- [ ] All 50 documents successfully processed without errors
- [ ] Pinecone index queryable via API
- [ ] Test queries return relevant chunks in top-5 results
- [ ] Metadata preserved correctly (source, author, section)

---

### Phase 3: Backend API Development (Week 2)

**Objective:** Build the core API routes for chat, sessions, and RAG orchestration.

**Tasks:**

| Task ID | Task Description                                                | Owner | Estimated Hours | Dependencies |
| ------- | --------------------------------------------------------------- | ----- | --------------- | ------------ |
| **3.1** | Create `/api/chat` endpoint (POST) for message handling         | Nani  | 4h              | 2.6          |
| **3.2** | Implement RAG orchestration logic (query ‚Üí retrieve ‚Üí generate) | Nani  | 6h              | 3.1          |
| **3.3** | Integrate OpenAI GPT-4o for response generation                 | Nani  | 3h              | 3.2          |
| **3.4** | Build conversation context manager (history + summarization)    | Nani  | 5h              | 3.2          |
| **3.5** | Add citation extraction and formatting logic                    | Nani  | 4h              | 3.2          |
| **3.6** | Create `/api/sessions` endpoint (GET, POST, DELETE)             | Nani  | 3h              | 1.7          |
| **3.7** | Implement mock authentication (`/api/auth/mock-login`)          | Nani  | 2h              | 3.6          |
| **3.8** | Add error handling and response validation                      | Nani  | 3h              | 3.2          |
| **3.9** | Write unit tests for RAG pipeline components                    | Nani  | 4h              | 3.8          |

**Deliverables:**

- ‚úÖ Working `/api/chat` endpoint accepting user messages
- ‚úÖ RAG pipeline retrieving and generating responses
- ‚úÖ Session management storing conversation history
- ‚úÖ Mock auth system simulating login
- ‚úÖ Unit tests with 80%+ coverage

**API Specifications (Detailed in Section 6)**

**Acceptance Criteria:**

- [ ] `/api/chat` returns response in < 3 seconds (p95)
- [ ] Citations included in 100% of substantive responses
- [ ] Conversation context maintained across 10+ turns
- [ ] Mock login creates valid session token
- [ ] All tests pass in CI pipeline

---

### Phase 4: Frontend UI Development (Week 2-3)

**Objective:** Build the user-facing chat interface with Solution Tree branding.

**Tasks:**

| Task ID  | Task Description                                               | Owner  | Estimated Hours | Dependencies |
| -------- | -------------------------------------------------------------- | ------ | --------------- | ------------ |
| **4.1**  | Design Solution Tree brand theme (colors, fonts, logos)        | Claude | 3h              | -            |
| **4.2**  | Build `ChatContainer` layout component                         | Nani   | 3h              | 4.1          |
| **4.3**  | Create `MessageList` with virtualized scrolling                | Nani   | 4h              | 4.2          |
| **4.4**  | Develop `MessageBubble` component (user vs. assistant styling) | Nani   | 3h              | 4.3          |
| **4.5**  | Build `ChatInput` with character counter and send button       | Nani   | 2h              | 4.2          |
| **4.6**  | Implement `TypingIndicator` animation                          | Nani   | 1h              | 4.3          |
| **4.7**  | Add citation display (inline + modal for details)              | Nani   | 4h              | 4.4          |
| **4.8**  | Build mock login screen with Solution Tree branding            | Nani   | 3h              | 4.1          |
| **4.9**  | Create responsive design for mobile (320px+)                   | Nani   | 4h              | 4.2          |
| **4.10** | Add loading states and error messages                          | Nani   | 2h              | 4.9          |
| **4.11** | Integrate frontend with `/api/chat` backend                    | Nani   | 3h              | 3.1, 4.2     |

**Deliverables:**

- ‚úÖ Fully functional chat interface
- ‚úÖ Solution Tree branded design system
- ‚úÖ Mobile-responsive layout
- ‚úÖ Mock login flow
- ‚úÖ Connected to backend API

**Design System:**

```typescript
// Solution Tree Brand Colors
const theme = {
  colors: {
    primary: "#0066CC", // Solution Tree blue
    secondary: "#FF6B35", // Accent orange
    background: "#F8F9FA", // Light gray
    surface: "#FFFFFF", // White
    text: {
      primary: "#1A1A1A",
      secondary: "#666666",
      disabled: "#CCCCCC",
    },
    status: {
      success: "#28A745",
      warning: "#FFC107",
      error: "#DC3545",
      info: "#17A2B8",
    },
  },
  typography: {
    fontFamily: "'Inter', -apple-system, sans-serif",
    fontSize: {
      xs: "0.75rem",
      sm: "0.875rem",
      base: "1rem",
      lg: "1.125rem",
      xl: "1.25rem",
      "2xl": "1.5rem",
    },
  },
};
```

**Acceptance Criteria:**

- [ ] Chat interface matches design mockups
- [ ] Messages send and display correctly
- [ ] Citations render with proper formatting
- [ ] Responsive on mobile devices (tested on iOS/Android)
- [ ] No console errors or warnings

---

### Phase 5: Testing & Quality Assurance (Week 3)

**Objective:** Comprehensive testing to ensure reliability, accuracy, and performance.

**Tasks:**

| Task ID | Task Description                                                    | Owner         | Estimated Hours | Dependencies |
| ------- | ------------------------------------------------------------------- | ------------- | --------------- | ------------ |
| **5.1** | Create 20 test coaching scenarios covering all 4 Critical Questions | Claude        | 6h              | -            |
| **5.2** | Run manual testing of each scenario and document results            | Nani + Claude | 8h              | 5.1          |
| **5.3** | Evaluate response accuracy with expert reviewer (if available)      | External      | 4h              | 5.2          |
| **5.4** | Write integration tests for `/api/chat` end-to-end flow             | Nani          | 5h              | 3.9          |
| **5.5** | Implement automated citation validation tests                       | Nani          | 3h              | 5.4          |
| **5.6** | Performance testing (response time, concurrent users)               | Nani          | 4h              | 5.4          |
| **5.7** | Fix bugs and refine prompts based on test results                   | Nani          | 8h              | 5.2, 5.3     |
| **5.8** | Conduct user acceptance testing (UAT) with 3-5 educators            | External      | 6h              | 5.7          |
| **5.9** | Create demo script and walkthrough video                            | Claude        | 4h              | 5.8          |

**Deliverables:**

- ‚úÖ 20 test scenarios with expected vs. actual results
- ‚úÖ Integration test suite with 90%+ coverage
- ‚úÖ Performance benchmark report
- ‚úÖ UAT feedback document
- ‚úÖ Polished demo walkthrough

**Test Scenario Example:**

| Scenario ID   | User Input                                                                 | Expected Outcome                                                                                                             | Pass/Fail |
| ------------- | -------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------- | --------- |
| **TS-Q3-001** | "Our 8th-grade math team scored 40% below proficiency on CFAs. What next?" | Coach asks clarifying questions (Which standard? What data protocol used?), then provides Q3-specific guidance with citation | ‚úÖ Pass   |

**Acceptance Criteria:**

- [ ] 85%+ of test scenarios produce accurate responses
- [ ] 100% of responses include valid citations
- [ ] Response time < 3s for 95th percentile
- [ ] UAT participants rate usability as 4.5/5.0+
- [ ] Demo video clearly communicates value proposition

---

### Phase 6: Polish & Deployment (Week 4)

**Objective:** Final refinements, documentation, and production deployment.

**Tasks:**

| Task ID  | Task Description                                     | Owner         | Estimated Hours | Dependencies |
| -------- | ---------------------------------------------------- | ------------- | --------------- | ------------ |
| **6.1**  | Finalize Solution Tree branding (logo, colors, copy) | Claude        | 3h              | -            |
| **6.2**  | Add analytics tracking (PostHog or similar)          | Nani          | 3h              | -            |
| **6.3**  | Implement feedback collection UI (thumbs up/down)    | Nani          | 3h              | 6.2          |
| **6.4**  | Create user onboarding flow (intro modal, tooltips)  | Nani          | 4h              | 6.1          |
| **6.5**  | Write comprehensive README and usage documentation   | Claude        | 4h              | -            |
| **6.6**  | Configure production environment variables on Vercel | Nani          | 1h              | 6.5          |
| **6.7**  | Run final security audit (API keys, rate limiting)   | Nani          | 3h              | 6.6          |
| **6.8**  | Deploy to Vercel production environment              | Nani          | 2h              | 6.7          |
| **6.9**  | Conduct smoke testing on production URL              | Nani + Claude | 2h              | 6.8          |
| **6.10** | Prepare investor/stakeholder demo presentation       | Claude        | 4h              | 6.9          |

**Deliverables:**

- ‚úÖ Production-ready application deployed to Vercel
- ‚úÖ Analytics dashboard configured
- ‚úÖ User documentation and README
- ‚úÖ Demo presentation materials
- ‚úÖ Security audit report

**Acceptance Criteria:**

- [ ] Production URL accessible and stable
- [ ] SSL certificate valid
- [ ] Analytics tracking events correctly
- [ ] Demo runs smoothly without errors
- [ ] All documentation complete and accurate

---

## 6. API Specifications

### 6.1 POST `/api/chat`

**Description:** Send a user message and receive an AI-generated coaching response.

**Request:**

```typescript
POST /api/chat
Content-Type: application/json
Authorization: Bearer <session_token>

{
  "sessionId": "uuid-v4",
  "message": "Our 8th-grade math team scored 40% below proficiency on CFAs. What next?",
  "metadata": {
    "timestamp": "2025-11-11T14:30:00Z",
    "source": "web" // or "mobile"
  }
}
```

**Response (Success):**

```typescript
200 OK
Content-Type: application/json

{
  "messageId": "uuid-v4",
  "role": "assistant",
  "content": "That's a common challenge, and it shows the power of your team's commitment to Question 2: *How will we know if each student has learned it?* Now, let's move with urgency to **Critical Question 3: *How will we respond when some students do not learn it?***\n\nThe most actionable next step is to **triage the data** and **plan a systematic intervention**.\n\n**Actionable Guidance:**\n1. **Identify the 'Why':** Use the item analysis from the CFA to pinpoint the specific skill or concept that caused the most difficulty.\n2. **Form Intervention Groups:** Based on the data, create small, flexible groups of students who need re-teaching on that specific skill.\n3. **Plan a Different Approach:** Discuss and document at least two instructional strategies that are *different* from the original teaching method. This is the core of a systematic response.\n\n[Source: Learning by Doing, Chapter 5]",
  "citations": [
    {
      "id": "cite-1",
      "sourceDocument": "Learning by Doing",
      "author": "DuFour et al.",
      "chapterOrSection": "Chapter 5: Critical Question 3",
      "pageNumber": 142,
      "relevanceScore": 0.92
    }
  ],
  "metadata": {
    "modelUsed": "gpt-4o",
    "tokensUsed": 1250,
    "responseTime": 2340, // milliseconds
    "retrievedChunks": 5
  },
  "timestamp": "2025-11-11T14:30:03Z"
}
```

**Response (Error):**

```typescript
400 Bad Request
Content-Type: application/json

{
  "error": "INVALID_REQUEST",
  "message": "Message exceeds maximum length of 2000 characters",
  "timestamp": "2025-11-11T14:30:00Z"
}
```

**Error Codes:**

- `400 INVALID_REQUEST` - Malformed request or validation error
- `401 UNAUTHORIZED` - Invalid or missing session token
- `429 RATE_LIMIT_EXCEEDED` - Too many requests (10/minute limit)
- `500 INTERNAL_SERVER_ERROR` - Unexpected server error
- `503 SERVICE_UNAVAILABLE` - OpenAI or Pinecone API unavailable

---

### 6.2 GET `/api/sessions/:sessionId`

**Description:** Retrieve conversation history for a given session.

**Request:**

```typescript
GET / api / sessions / uuid - v4;
Authorization: Bearer<session_token>;
```

**Response:**

```typescript
200 OK
Content-Type: application/json

{
  "sessionId": "uuid-v4",
  "userId": "mock-user-id",
  "startedAt": "2025-11-11T14:00:00Z",
  "lastActiveAt": "2025-11-11T14:30:03Z",
  "summary": "Team discussing intervention strategies for 8th-grade math CFA results showing 40% below proficiency.",
  "messages": [
    {
      "id": "msg-1",
      "role": "user",
      "content": "Our 8th-grade math team scored 40% below proficiency on CFAs. What next?",
      "timestamp": "2025-11-11T14:30:00Z"
    },
    {
      "id": "msg-2",
      "role": "assistant",
      "content": "That's a common challenge...",
      "timestamp": "2025-11-11T14:30:03Z",
      "citations": [...]
    }
  ],
  "metadata": {
    "criticalQuestionsFocused": [2, 3],
    "topicsDiscussed": ["CFAs", "interventions", "data analysis"],
    "documentsReferenced": ["Learning by Doing"]
  }
}
```

---

### 6.3 POST `/api/sessions`

**Description:** Create a new coaching session.

**Request:**

```typescript
POST /api/sessions
Content-Type: application/json
Authorization: Bearer <session_token>

{
  "userId": "mock-user-id",
  "initialContext": {
    "userRole": "PLC Team Leader",
    "schoolLevel": "Middle School",
    "focusArea": "Math CFAs"
  }
}
```

**Response:**

```typescript
201 Created
Content-Type: application/json

{
  "sessionId": "uuid-v4",
  "userId": "mock-user-id",
  "startedAt": "2025-11-11T14:00:00Z",
  "status": "active"
}
```

---

### 6.4 POST `/api/feedback`

**Description:** Submit user feedback on a specific message.

**Request:**

```typescript
POST /api/feedback
Content-Type: application/json
Authorization: Bearer <session_token>

{
  "messageId": "msg-2",
  "sessionId": "uuid-v4",
  "rating": "helpful", // or "not_helpful"
  "feedbackText": "Very actionable! Helped us plan interventions.",
  "followUpAction": "Created intervention groups as suggested"
}
```

**Response:**

```typescript
200 OK
Content-Type: application/json

{
  "feedbackId": "uuid-v4",
  "status": "recorded",
  "timestamp": "2025-11-11T14:35:00Z"
}
```

---

## 7. UI/UX Design Guidelines

### 7.1 Solution Tree Brand Identity

**Logo Usage:**

- Place Solution Tree logo in top-left corner (40px height)
- Use official logo files (SVG preferred for scalability)
- Link logo to homepage or demo landing page

**Color Palette:**

```css
:root {
  /* Primary Colors */
  --st-blue-primary: #0066cc;
  --st-blue-dark: #004c99;
  --st-blue-light: #3385d6;

  /* Accent Colors */
  --st-orange: #ff6b35;
  --st-green: #28a745;

  /* Neutral Colors */
  --st-gray-50: #f8f9fa;
  --st-gray-100: #e9ecef;
  --st-gray-300: #ced4da;
  --st-gray-700: #495057;
  --st-gray-900: #1a1a1a;

  /* Semantic Colors */
  --st-success: #28a745;
  --st-warning: #ffc107;
  --st-error: #dc3545;
  --st-info: #17a2b8;
}
```

**Typography:**

- **Headings:** Inter (Bold, 600 weight)
- **Body:** Inter (Regular, 400 weight)
- **Code/Monospace:** 'Fira Code' or 'SF Mono'

**Spacing Scale (Tailwind):**

- Use consistent spacing: 4px, 8px, 12px, 16px, 24px, 32px, 48px, 64px

---

### 7.2 Chat Interface Design

**Layout Structure:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  [Logo]  AI-Powered PLC Coach    [User Avatar] ‚îÇ  <- Header (64px)
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  User: Our 8th grade math team...       ‚îÇ   ‚îÇ  <- Message (User)
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  Coach: That's a common challenge...    ‚îÇ   ‚îÇ  <- Message (Assistant)
‚îÇ  ‚îÇ  [Source: Learning by Doing]            ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                 ‚îÇ
‚îÇ  [Coach is thinking...]                         ‚îÇ  <- Typing Indicator
‚îÇ                                                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  [Type your message here...]      [Send ‚Üí]     ‚îÇ  <- Input (72px)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Message Bubble Styling:**

**User Messages:**

- Background: `--st-blue-primary` (#0066CC)
- Text: White
- Border-radius: 16px 16px 4px 16px
- Align: Right
- Max-width: 70%
- Shadow: `0 2px 8px rgba(0, 102, 204, 0.2)`

**Assistant Messages:**

- Background: White
- Text: `--st-gray-900`
- Border: 1px solid `--st-gray-300`
- Border-radius: 16px 16px 16px 4px
- Align: Left
- Max-width: 85%
- Shadow: `0 2px 8px rgba(0, 0, 0, 0.08)`

**Citation Display:**

- Inline citations: Blue pill with book icon
- Hover: Shows tooltip with full source details
- Click: Opens modal with source excerpt and metadata

---

### 7.3 Component Specifications

#### 7.3.1 ChatContainer

```tsx
// app/components/ChatContainer.tsx
import { useState, useEffect, useRef } from "react";
import { MessageList } from "./MessageList";
import { ChatInput } from "./ChatInput";
import { TypingIndicator } from "./TypingIndicator";

interface Message {
  id: string;
  role: "user" | "assistant";
  content: string;
  timestamp: Date;
  citations?: Citation[];
}

export function ChatContainer({ sessionId }: { sessionId: string }) {
  const [messages, setMessages] = useState<Message[]>([]);
  const [isLoading, setIsLoading] = useState(false);
  const scrollRef = useRef<HTMLDivElement>(null);

  const handleSendMessage = async (content: string) => {
    const userMessage: Message = {
      id: crypto.randomUUID(),
      role: "user",
      content,
      timestamp: new Date(),
    };

    setMessages((prev) => [...prev, userMessage]);
    setIsLoading(true);

    try {
      const response = await fetch("/api/chat", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ sessionId, message: content }),
      });

      const assistantMessage = await response.json();
      setMessages((prev) => [...prev, assistantMessage]);
    } catch (error) {
      console.error("Failed to send message:", error);
      // Show error toast
    } finally {
      setIsLoading(false);
    }
  };

  useEffect(() => {
    // Auto-scroll to bottom on new messages
    scrollRef.current?.scrollIntoView({ behavior: "smooth" });
  }, [messages]);

  return (
    <div className="flex flex-col h-screen bg-gray-50">
      <MessageList messages={messages} />
      {isLoading && <TypingIndicator />}
      <div ref={scrollRef} />
      <ChatInput onSend={handleSendMessage} disabled={isLoading} />
    </div>
  );
}
```

#### 7.3.2 MessageBubble

```tsx
// app/components/MessageBubble.tsx
import { Message } from "@/types";
import { CitationPill } from "./CitationPill";

export function MessageBubble({ message }: { message: Message }) {
  const isUser = message.role === "user";

  return (
    <div className={`flex ${isUser ? "justify-end" : "justify-start"} mb-4`}>
      <div
        className={`
          max-w-[85%] px-4 py-3 rounded-2xl
          ${
            isUser
              ? "bg-st-blue-primary text-white rounded-br-sm"
              : "bg-white border border-gray-300 rounded-bl-sm shadow-sm"
          }
        `}
      >
        <p className="text-sm leading-relaxed whitespace-pre-wrap">
          {message.content}
        </p>

        {message.citations && message.citations.length > 0 && (
          <div className="flex flex-wrap gap-2 mt-3">
            {message.citations.map((citation) => (
              <CitationPill key={citation.id} citation={citation} />
            ))}
          </div>
        )}

        <span className="text-xs opacity-60 mt-2 block">
          {new Date(message.timestamp).toLocaleTimeString()}
        </span>
      </div>
    </div>
  );
}
```

#### 7.3.3 ChatInput

```tsx
// app/components/ChatInput.tsx
import { useState } from "react";
import { Send } from "lucide-react";

interface ChatInputProps {
  onSend: (message: string) => void;
  disabled?: boolean;
}

export function ChatInput({ onSend, disabled }: ChatInputProps) {
  const [input, setInput] = useState("");
  const maxLength = 2000;

  const handleSubmit = (e: React.FormEvent) => {
    e.preventDefault();
    if (input.trim() && !disabled) {
      onSend(input.trim());
      setInput("");
    }
  };

  return (
    <form
      onSubmit={handleSubmit}
      className="border-t border-gray-300 bg-white p-4 flex gap-2"
    >
      <textarea
        value={input}
        onChange={(e) => setInput(e.target.value.slice(0, maxLength))}
        placeholder="Ask the PLC Coach anything..."
        disabled={disabled}
        className="
          flex-1 resize-none border border-gray-300 rounded-lg px-4 py-2
          focus:outline-none focus:ring-2 focus:ring-st-blue-primary
          disabled:opacity-50 disabled:cursor-not-allowed
        "
        rows={2}
        onKeyDown={(e) => {
          if (e.key === "Enter" && !e.shiftKey) {
            e.preventDefault();
            handleSubmit(e);
          }
        }}
      />

      <div className="flex flex-col justify-between">
        <button
          type="submit"
          disabled={!input.trim() || disabled}
          className="
            bg-st-blue-primary text-white p-3 rounded-lg
            hover:bg-st-blue-dark transition-colors
            disabled:opacity-50 disabled:cursor-not-allowed
          "
        >
          <Send size={20} />
        </button>
        <span className="text-xs text-gray-500 mt-1">
          {input.length}/{maxLength}
        </span>
      </div>
    </form>
  );
}
```

---

### 7.4 Responsive Design

**Breakpoints:**

```css
/* Mobile */
@media (min-width: 320px) {
  /* Stack vertically, larger touch targets */
}

/* Tablet */
@media (min-width: 768px) {
  /* Sidebar navigation, two-column layouts */
}

/* Desktop */
@media (min-width: 1024px) {
  /* Three-column layouts, fixed sidebars */
}
```

**Mobile Optimizations:**

- Larger touch targets (48px minimum)
- Fixed header with hamburger menu
- Bottom-anchored chat input
- Swipe gestures for navigation
- Reduced animations for performance

---

### 7.5 Accessibility (WCAG 2.1 AA)

**Requirements:**

- ‚úÖ Color contrast ratio ‚â• 4.5:1 for normal text
- ‚úÖ Keyboard navigation for all interactive elements
- ‚úÖ ARIA labels for screen readers
- ‚úÖ Focus indicators visible (2px outline)
- ‚úÖ Alt text for all images and icons
- ‚úÖ Semantic HTML (proper heading hierarchy)

**Example:**

```tsx
<button
  onClick={handleSend}
  aria-label="Send message to AI coach"
  className="focus:ring-2 focus:ring-offset-2 focus:ring-st-blue-primary"
>
  <Send aria-hidden="true" />
  <span className="sr-only">Send</span>
</button>
```

---

## 8. Knowledge Base Strategy

### 8.1 Document Sourcing

**Public Sources (No Copyright Issues):**

1. **Solution Tree Public Resources:**
   - Blog posts from AllThingsPLC.info
   - Free guides and white papers
   - Publicly available webinar transcripts
2. **Open Educational Resources:**
   - Creative Commons licensed PLC guides
   - Public school district PLC implementation plans
   - Academic research papers on PLCs (ERIC database)
3. **Mock Content Generation:**
   - Use Claude to create realistic PLC coaching scenarios
   - Generate synthetic CFA data and team artifacts
   - Write mock case studies based on PLC framework

**Content Mix Target:**

| Category                  | Count | Source Strategy                                                       |
| ------------------------- | ----- | --------------------------------------------------------------------- |
| **Core Framework Docs**   | 10    | Mock documents explaining Three Big Ideas and Four Critical Questions |
| **Coaching Transcripts**  | 15    | Claude-generated realistic coaching conversations                     |
| **Implementation Guides** | 10    | Public resources + enhanced with PLC terminology                      |
| **Research Articles**     | 10    | Open-access academic papers on PLCs                                   |
| **Case Studies**          | 5     | Anonymized real-world examples + mock scenarios                       |

---

### 8.2 Document Preparation Checklist

**Pre-Processing Steps:**

- [ ] Convert all documents to plain text (Markdown or TXT)
- [ ] Remove extraneous content (ads, navigation, footers)
- [ ] Add structured metadata header to each document
- [ ] Validate formatting (headings, lists, tables)
- [ ] Check for sensitive information (PII, proprietary data)

**Metadata Template:**

```markdown
---
title: "Learning by Doing: Chapter 5 - Critical Question 3"
author: "DuFour, DuFour, Eaker, Many"
type: "book_chapter"
critical_question: 3
topics: ["intervention", "response to learning", "systematic approach"]
publication_year: 2016
source_url: "https://www.solutiontree.com/learning-by-doing"
---

[Document content begins here...]
```

---

### 8.3 Chunking Strategy

**Chunk Size Optimization:**

- **Target:** 500 tokens per chunk (~2000 characters)
- **Overlap:** 50 tokens to preserve context across boundaries
- **Max Chunk Size:** 800 tokens (prevents embedding quality degradation)
- **Min Chunk Size:** 200 tokens (ensures semantic completeness)

**Chunking Logic:**

```python
def intelligent_chunker(document: str, metadata: dict) -> List[Chunk]:
    """
    Chunks document while respecting semantic boundaries.
    Priority: Paragraphs > Sentences > Fixed-size
    """
    chunks = []

    # Split by double newlines (paragraphs)
    paragraphs = document.split('\n\n')

    current_chunk = ""
    current_tokens = 0

    for para in paragraphs:
        para_tokens = count_tokens(para)

        # If paragraph alone exceeds max, split by sentences
        if para_tokens > 800:
            sentences = split_sentences(para)
            for sent in sentences:
                sent_tokens = count_tokens(sent)

                if current_tokens + sent_tokens > 500:
                    # Save current chunk
                    chunks.append(create_chunk(current_chunk, metadata))

                    # Start new chunk with overlap
                    overlap_text = get_last_n_tokens(current_chunk, 50)
                    current_chunk = overlap_text + " " + sent
                    current_tokens = count_tokens(current_chunk)
                else:
                    current_chunk += " " + sent
                    current_tokens += sent_tokens

        # Normal case: add paragraph to chunk
        elif current_tokens + para_tokens <= 500:
            current_chunk += "\n\n" + para
            current_tokens += para_tokens

        # Paragraph would exceed limit: save and start new
        else:
            chunks.append(create_chunk(current_chunk, metadata))
            overlap_text = get_last_n_tokens(current_chunk, 50)
            current_chunk = overlap_text + "\n\n" + para
            current_tokens = count_tokens(current_chunk)

    # Save final chunk
    if current_chunk:
        chunks.append(create_chunk(current_chunk, metadata))

    return chunks
```

---

### 8.4 Metadata Schema

**Required Fields:**

```typescript
interface ChunkMetadata {
  // Document-level
  sourceDocument: string; // "Learning by Doing"
  author: string; // "DuFour et al."
  documentType: "book" | "article" | "guide" | "case_study" | "faq";
  publicationYear: number;

  // Chunk-level
  chunkId: string; // "lbd-ch5-001"
  section: string; // "Chapter 5: Systematic Interventions"
  pageNumber?: number;

  // PLC Framework Alignment
  criticalQuestion?: 1 | 2 | 3 | 4;
  bigIdea?: "learning" | "collaboration" | "results";
  topics: string[]; // ["intervention", "tier 2", "progress monitoring"]

  // Technical
  tokenCount: number;
  createdAt: string; // ISO timestamp
}
```

---

### 8.5 Embedding Generation

**OpenAI Embedding Model:**

- **Model:** `text-embedding-3-large`
- **Dimensions:** 3072 (higher quality than `text-embedding-3-small`)
- **Cost:** $0.13 / 1M tokens
- **Estimated Total Cost:** ~$2.50 for 2,500 chunks (50 docs)

**Batch Processing:**

```typescript
async function generateEmbeddings(chunks: Chunk[]): Promise<EmbeddedChunk[]> {
  const batchSize = 100; // OpenAI supports up to 2048 inputs per request
  const embeddedChunks: EmbeddedChunk[] = [];

  for (let i = 0; i < chunks.length; i += batchSize) {
    const batch = chunks.slice(i, i + batchSize);

    const response = await openai.embeddings.create({
      model: "text-embedding-3-large",
      input: batch.map((c) => c.content),
      dimensions: 3072,
    });

    batch.forEach((chunk, idx) => {
      embeddedChunks.push({
        ...chunk,
        embedding: response.data[idx].embedding,
      });
    });

    console.log(`Processed ${i + batch.length} / ${chunks.length} chunks`);
    await sleep(1000); // Rate limiting: 1 second between batches
  }

  return embeddedChunks;
}
```

---

### 8.6 Pinecone Index Configuration

**Index Setup:**

```typescript
import { Pinecone } from "@pinecone-database/pinecone";

const pinecone = new Pinecone({
  apiKey: process.env.PINECONE_API_KEY!,
});

// Create index (run once)
await pinecone.createIndex({
  name: "plc-coach-demo",
  dimension: 3072,
  metric: "cosine", // Best for semantic similarity
  spec: {
    serverless: {
      cloud: "aws",
      region: "us-east-1", // Choose closest to Vercel edge
    },
  },
});

// Get index reference
const index = pinecone.index("plc-coach-demo");
```

**Upsert Vectors:**

```typescript
async function uploadToPinecone(embeddedChunks: EmbeddedChunk[]) {
  const index = pinecone.index("plc-coach-demo");

  // Pinecone supports up to 100 vectors per upsert
  const batchSize = 100;

  for (let i = 0; i < embeddedChunks.length; i += batchSize) {
    const batch = embeddedChunks.slice(i, i + batchSize);

    const vectors = batch.map((chunk) => ({
      id: chunk.chunkId,
      values: chunk.embedding,
      metadata: {
        content: chunk.content,
        sourceDocument: chunk.metadata.sourceDocument,
        author: chunk.metadata.author,
        section: chunk.metadata.section,
        criticalQuestion: chunk.metadata.criticalQuestion,
        topics: chunk.metadata.topics.join(", "),
      },
    }));

    await index.upsert(vectors);
    console.log(
      `Uploaded ${i + batch.length} / ${embeddedChunks.length} vectors`
    );
  }
}
```

---

## 9. Testing Strategy

### 9.1 Test Scenario Matrix

**Coverage by Critical Question:**

| Critical Question                                  | Test Scenarios                                       | Priority |
| -------------------------------------------------- | ---------------------------------------------------- | -------- |
| **Q1: What do we want students to learn?**         | 5 scenarios (curriculum alignment, learning targets) | High     |
| **Q2: How will we know if they learned it?**       | 7 scenarios (CFA analysis, data protocols)           | Critical |
| **Q3: How will we respond when they don't learn?** | 6 scenarios (interventions, RTI)                     | Critical |
| **Q4: How will we extend learning?**               | 2 scenarios (enrichment, differentiation)            | Medium   |

**Total:** 20 test scenarios

---

### 9.2 Sample Test Scenarios

#### Test Scenario Q2-001: First CFA Data Review

**Category:** Critical Question 2 (Assessment)

**User Input:**

```
"Our 7th-grade ELA team just completed our first Common Formative Assessment
on argumentative writing. 22 out of 75 students scored below proficiency (29%).
We're not sure what to do next."
```

**Expected Coach Response Elements:**

- ‚úÖ Acknowledges this is a Q2 ‚Üí Q3 transition
- ‚úÖ Asks clarifying questions about the CFA design (e.g., "What proficiency criteria did you use?")
- ‚úÖ Guides team to use a Data Protocol
- ‚úÖ Mentions the importance of item analysis
- ‚úÖ References specific Solution Tree resource (e.g., _Collaborative Common Assessments_)
- ‚úÖ Provides actionable next steps
- ‚úÖ Includes at least 1 citation

**Success Criteria:**

- Response addresses ‚â•5 of the expected elements
- Citation is valid and relevant
- Tone is facilitative, not prescriptive
- Response delivered in <3 seconds

---

#### Test Scenario Q3-004: Intervention Planning

**Category:** Critical Question 3 (Intervention)

**User Input:**

```
"We've identified 12 students who need help with multiplying fractions.
Should we pull them out during math class or do it after school?"
```

**Expected Coach Response Elements:**

- ‚úÖ Recognizes this as a Q3 implementation challenge
- ‚úÖ Asks about current instruction during core math time
- ‚úÖ References the "systematic" principle (not punitive, timely)
- ‚úÖ Suggests considering both Tier 2 (supplemental) and scheduling logistics
- ‚úÖ Mentions the importance of progress monitoring
- ‚úÖ Cites _RTI at Work_ or similar intervention resource

**Success Criteria:**

- Provides guidance on scheduling interventions
- Emphasizes systematic approach over ad-hoc solutions
- Does not give a binary "yes/no" answer without context
- Citation relevant to intervention design

---

### 9.3 Automated Test Suite

**Unit Tests:**

```typescript
// __tests__/api/chat.test.ts
import { POST } from "@/app/api/chat/route";
import { testScenarios } from "./fixtures/test-scenarios";

describe("POST /api/chat", () => {
  it("should return response with citation for Q3 scenario", async () => {
    const request = new Request("http://localhost:3000/api/chat", {
      method: "POST",
      body: JSON.stringify({
        sessionId: "test-session",
        message: testScenarios["Q3-004"].input,
      }),
    });

    const response = await POST(request);
    const data = await response.json();

    expect(data.content).toBeTruthy();
    expect(data.citations).toHaveLength.greaterThan(0);
    expect(data.citations[0].sourceDocument).toBeTruthy();
    expect(data.metadata.responseTime).toBeLessThan(3000);
  });

  it("should handle conversation context across multiple turns", async () => {
    // Test multi-turn conversation
    const session = await createTestSession();

    // Turn 1
    await sendMessage(session.id, "Our CFA shows 40% below proficiency");

    // Turn 2 (should remember context)
    const response = await sendMessage(
      session.id,
      "What interventions work best?"
    );

    expect(response.content).toContain("40%"); // Context retained
  });
});
```

**Integration Tests:**

```typescript
// __tests__/integration/rag-pipeline.test.ts
describe("RAG Pipeline Integration", () => {
  it("should retrieve relevant chunks for intervention query", async () => {
    const query = "How do we plan systematic interventions?";

    const retrievedChunks = await retrieveContext(query, []);

    expect(retrievedChunks).toHaveLength(5);
    expect(retrievedChunks[0].metadata.criticalQuestion).toBe(3);
    expect(retrievedChunks[0].metadata.topics).toContain("intervention");
  });

  it("should generate response with valid citations", async () => {
    const query = "What is Question 3 about?";

    const response = await generateResponse(query, []);

    expect(response.content).toMatch(/\[Source:.*\]/);
    expect(response.citations[0].sourceDocument).toBeTruthy();
  });
});
```

---

### 9.4 Performance Benchmarks

**Target Metrics:**

| Metric                  | Target | Measurement                           |
| ----------------------- | ------ | ------------------------------------- |
| **Response Time (p50)** | <2s    | Time from user message to first token |
| **Response Time (p95)** | <3s    | 95th percentile response time         |
| **Response Time (p99)** | <5s    | 99th percentile response time         |
| **Retrieval Time**      | <200ms | Pinecone query duration               |
| **LLM Generation Time** | <2s    | GPT-4o completion time                |
| **Concurrent Users**    | 10+    | Simultaneous chat sessions            |

**Load Testing:**

```bash
# Using k6 for load testing
k6 run load-test.js

# Expected output:
# http_req_duration..........: avg=2.1s  p(95)=2.8s  p(99)=4.2s
# http_req_failed............: 0.00%    (0 failures)
# vus........................: 10       (10 concurrent users)
# vus_max....................: 10
```

---

### 9.5 User Acceptance Testing (UAT)

**Participants:**

- 3-5 educators (PLC team leaders, instructional coaches)
- At least 1 Solution Tree expert (if available)

**UAT Protocol:**

1. **Pre-Test Survey:**

   - Current PLC experience level
   - Expectations for AI coaching

2. **Guided Demo (10 minutes):**

   - Walk through 2-3 pre-scripted scenarios
   - Show key features (citations, context retention)

3. **Free Exploration (15 minutes):**

   - Participants ask their own questions
   - Observe natural interaction patterns

4. **Post-Test Survey:**
   - Likert scale ratings (1-5):
     - Response relevance
     - Coaching tone appropriateness
     - Citation usefulness
     - Likelihood to use in practice
   - Open-ended feedback

**Success Criteria:**

- Average rating ‚â• 4.5/5.0 across all dimensions
- 100% of participants agree: "Coaching felt authentic to PLC framework"
- ‚â•80% say: "I would use this tool in my PLC work"

---

## 10. Success Metrics & KPIs

### 10.1 Technical Performance KPIs

| KPI                         | Target | Current | Status     |
| --------------------------- | ------ | ------- | ---------- |
| **Uptime**                  | 99.5%  | TBD     | üü° Pending |
| **Response Time (p95)**     | <3s    | TBD     | üü° Pending |
| **Citation Accuracy**       | 100%   | TBD     | üü° Pending |
| **RAG Retrieval Relevance** | 85%+   | TBD     | üü° Pending |
| **Error Rate**              | <1%    | TBD     | üü° Pending |

---

### 10.2 Product Quality KPIs

| KPI                    | Target                                     | Measurement Method                     |
| ---------------------- | ------------------------------------------ | -------------------------------------- |
| **Response Accuracy**  | 85%+ relevant                              | Expert evaluation of 20 test scenarios |
| **Framework Fidelity** | 90%+ responses reference PLC framework     | Automated text analysis                |
| **Coaching Tone**      | 4.5/5.0 rating                             | UAT survey question                    |
| **User Satisfaction**  | 4.5/5.0 average                            | Post-demo survey (5-point Likert)      |
| **Actionability**      | 80%+ responses include specific next steps | Manual review by experts               |

---

### 10.3 Demo Success Metrics

**Stakeholder Demo Goals:**

| Audience                     | Success Criteria                                                                                                         |
| ---------------------------- | ------------------------------------------------------------------------------------------------------------------------ |
| **Solution Tree Leadership** | - Demo runs smoothly without errors<br>- Coaching tone matches their standards<br>- Interest in pilot program            |
| **Investors**                | - Clear value proposition communicated<br>- Defensible technical moat (RAG + framework)<br>- Follow-up meeting scheduled |
| **Educators (UAT)**          | - 80%+ would use in practice<br>- Positive word-of-mouth<br>- Feature requests for v2                                    |

---

## 11. Risk Mitigation

### 11.1 Technical Risks

| Risk                           | Probability | Impact | Mitigation Strategy                                                                                                          |
| ------------------------------ | ----------- | ------ | ---------------------------------------------------------------------------------------------------------------------------- |
| **OpenAI API downtime**        | Medium      | High   | - Implement retry logic with exponential backoff<br>- Cache common responses<br>- Fallback to pre-written guidance           |
| **Pinecone rate limits**       | Low         | Medium | - Use free tier efficiently<br>- Cache retrieval results<br>- Implement request throttling                                   |
| **Poor RAG retrieval quality** | Medium      | High   | - Test extensively with diverse queries<br>- Refine chunking strategy<br>- Add hybrid search (keyword + vector)              |
| **Slow response times**        | Medium      | Medium | - Optimize chunk size<br>- Use streaming responses<br>- Deploy to optimal Vercel edge locations                              |
| **Citation hallucinations**    | Low         | High   | - Post-process to validate citations<br>- Use structured output format<br>- Implement citation blocklist for invalid sources |

---

### 11.2 Product Risks

| Risk                                 | Probability | Impact | Mitigation Strategy                                                                                                                    |
| ------------------------------------ | ----------- | ------ | -------------------------------------------------------------------------------------------------------------------------------------- |
| **Coaching tone misalignment**       | Medium      | High   | - Extensive prompt engineering<br>- Expert review of responses<br>- A/B test different persona prompts                                 |
| **Limited knowledge base**           | High        | Medium | - Prioritize quality over quantity (50 curated docs)<br>- Mock additional content as needed<br>- Clearly scope demo limitations        |
| **User confusion with mock auth**    | Low         | Low    | - Clear "Demo Mode" banner<br>- Skip auth for demo presentation<br>- Provide instructions in README                                    |
| **Educators find responses generic** | Medium      | High   | - Ground every response in PLC framework<br>- Use specific examples and case studies<br>- Avoid AI clich√©s ("I'd be happy to help...") |

---

### 11.3 Timeline Risks

| Risk                             | Probability | Impact | Mitigation Strategy                                                                                                             |
| -------------------------------- | ----------- | ------ | ------------------------------------------------------------------------------------------------------------------------------- |
| **Phase overruns**               | High        | Medium | - Ruthlessly prioritize MVP features<br>- Cut enhanced features if needed<br>- Timebox each phase                               |
| **Knowledge base prep delays**   | Medium      | Medium | - Start content curation early (Week 1)<br>- Use Claude to accelerate mock content generation<br>- Parallelize with development |
| **Testing reveals major issues** | Medium      | High   | - Test incrementally (don't wait until Phase 5)<br>- Build in buffer time (1 week)<br>- Have fallback simplified demo           |

---

## 12. Appendix

### 12.1 Environment Variables

```bash
# .env.local (local development)
OPENAI_API_KEY=sk-...
PINECONE_API_KEY=...
PINECONE_INDEX_NAME=plc-coach-demo
PINECONE_ENVIRONMENT=us-east-1
DATABASE_URL=postgresql://...
SESSION_SECRET=...
NEXT_PUBLIC_APP_URL=http://localhost:3000

# Vercel Production
# Configure these in Vercel dashboard under Settings > Environment Variables
```

---

### 12.2 Project Structure

```
plc-virtual-coach/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ (auth)/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ login/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ page.tsx              # Mock login page
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ layout.tsx
‚îÇ   ‚îú‚îÄ‚îÄ (chat)/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ page.tsx                  # Main chat interface
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ layout.tsx
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chat/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ route.ts              # POST /api/chat
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sessions/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ route.ts              # POST /api/sessions
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ [id]/route.ts         # GET /api/sessions/:id
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ feedback/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ route.ts              # POST /api/feedback
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ auth/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ mock-login/route.ts   # Mock authentication
‚îÇ   ‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ChatContainer.tsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ MessageList.tsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ MessageBubble.tsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ChatInput.tsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ TypingIndicator.tsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ CitationPill.tsx
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ CitationModal.tsx
‚îÇ   ‚îú‚îÄ‚îÄ lib/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rag/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ retrieval.ts          # Pinecone query logic
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ generation.ts         # OpenAI completion
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ orchestrator.ts       # RAG pipeline
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ conversation/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ manager.ts            # Context management
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ summarizer.ts         # Conversation summarization
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ db/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schema.ts             # Database schema
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ client.ts             # Database client
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ citations.ts          # Citation formatting
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ validation.ts         # Input validation
‚îÇ   ‚îú‚îÄ‚îÄ types/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ message.ts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ citation.ts
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ session.ts
‚îÇ   ‚îî‚îÄ‚îÄ globals.css
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ index_knowledge_base.ts       # RAG indexing script
‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pdf_parser.ts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ markdown_parser.ts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chunker.ts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ metadata_extractor.ts
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ embeddings.ts
‚îÇ   ‚îî‚îÄ‚îÄ knowledge_base/               # 50+ source documents
‚îÇ       ‚îú‚îÄ‚îÄ core_texts/
‚îÇ       ‚îú‚îÄ‚îÄ articles/
‚îÇ       ‚îú‚îÄ‚îÄ guides/
‚îÇ       ‚îú‚îÄ‚îÄ case_studies/
‚îÇ       ‚îî‚îÄ‚îÄ faq/
‚îú‚îÄ‚îÄ __tests__/
‚îÇ   ‚îú‚îÄ‚îÄ unit/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rag.test.ts
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ conversation.test.ts
‚îÇ   ‚îú‚îÄ‚îÄ integration/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ api-chat.test.ts
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ rag-pipeline.test.ts
‚îÇ   ‚îî‚îÄ‚îÄ e2e/
‚îÇ       ‚îî‚îÄ‚îÄ chat-flow.test.ts
‚îú‚îÄ‚îÄ public/
‚îÇ   ‚îú‚îÄ‚îÄ solution-tree-logo.svg
‚îÇ   ‚îî‚îÄ‚îÄ assets/
‚îú‚îÄ‚îÄ docker-compose.yml                # Local development environment
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ .env.local.example
‚îú‚îÄ‚îÄ package.json
‚îú‚îÄ‚îÄ tsconfig.json
‚îú‚îÄ‚îÄ tailwind.config.ts
‚îú‚îÄ‚îÄ next.config.js
‚îî‚îÄ‚îÄ README.md
```

---

### 12.3 Glossary

| Term                            | Definition                                                                                                                   |
| ------------------------------- | ---------------------------------------------------------------------------------------------------------------------------- |
| **CFA**                         | Common Formative Assessment - a team-designed assessment used to measure student progress toward essential learning outcomes |
| **Critical Question (CQ)**      | One of the four foundational questions that guide PLC work                                                                   |
| **Collaborative Team**          | A group of educators working interdependently to achieve common goals for which members are mutually accountable             |
| **RAG**                         | Retrieval-Augmented Generation - AI technique combining vector search with LLM generation                                    |
| **Systematic Intervention**     | Timely, directive, non-punitive support for struggling learners (not "sink or swim")                                         |
| **Essential Learning Outcomes** | The guaranteed and viable curriculum - what all students must learn                                                          |
| **Data Protocol**               | A structured process for collaborative analysis of student learning evidence                                                 |

---

### 12.4 References

1. DuFour, R., DuFour, R., Eaker, R., Many, T., & Mattos, M. (2016). _Learning by Doing: A Handbook for Professional Learning Communities at Work_ (3rd ed.). Solution Tree Press.

2. AllThingsPLC. (n.d.). _What is a Professional Learning Community?_ https://www.allthingsplc.info/

3. Solution Tree. (2025). _PLC at Work¬Æ: Why PLC at Work¬Æ_. https://www.solutiontree.com/plc-at-work/why-plc-at-work

4. Erkens, C., Schimmer, T., & Vagle, N. D. (2017). _Essential Assessment: Six Tenets for Bringing Hope, Efficacy, and Achievement to the Classroom_. Solution Tree Press.

5. Buffum, A., Mattos, M., & Weber, C. (2012). _Simplifying Response to Intervention: Four Essential Guiding Principles_. Solution Tree Press.

---

### 12.5 Revision History

| Version | Date       | Author        | Changes             |
| ------- | ---------- | ------------- | ------------------- |
| 1.0     | 2025-11-11 | Claude + Nani | Initial PRD created |

---

**END OF DOCUMENT**

---

## Next Steps for Implementation

1. **Review & Approve:** Review this PRD and provide feedback on any sections that need adjustment
2. **Environment Setup:** Begin Phase 1 tasks (Next.js setup, Vercel deployment)
3. **Content Curation:** Start gathering and preparing the 50+ knowledge base documents
4. **Kickoff Development:** Begin coding with Phase 2 (RAG indexing)

**Questions for Nani:**

- Does the technical stack (Next.js + Pinecone + OpenAI) align with your vision?
- Are there any features in the "Enhanced" section you'd like to prioritize for MVP?
- Should we schedule a kick-off call to discuss implementation details?
